# Learn Like Magic - Book Ingestion & Guideline Generation
# Implementation Plan

## Document Information
- **Created:** 2025-10-25
- **Status:** Draft - Awaiting AWS Credentials & OpenAI Vision Reference Code
- **Target:** MVP for CBSE Grade 3 Mathematics (NCERT Math Magic)

---

## 1. Executive Summary

This implementation plan details the technical approach to build the Book Ingestion & Guideline Generation system for Learn Like Magic. The system will enable admins to:

1. Upload textbook pages as images
2. Extract text via OpenAI Vision OCR
3. Review and approve OCR output
4. Generate teaching guidelines using LangGraph
5. Automatically populate the existing `teaching_guidelines` table

**Key Architecture Decisions:**
- S3 for file storage (`learnlikemagic-books` bucket)
- PostgreSQL for metadata and state tracking
- Admin UI embedded in existing React frontend
- LangGraph for guideline extraction workflow
- Subtopic-level granularity maintained in teaching_guidelines table

---

## 2. Implementation Phases

### Phase 1: Database Schema & Migrations (Can start immediately)
**Duration:** 1-2 days
**Blockers:** None

**Tasks:**
1. Create database migration scripts (Alembic)
2. Add new tables: `books`, `book_guidelines`
3. Modify existing table: `teaching_guidelines` (add `book_id`, `source_pages` columns)
4. Update ORM models in `models/database.py`
5. Update Pydantic schemas in `models/schemas.py`
6. Create repository classes for new tables

**Deliverables:**
- Migration files in `llm-backend/alembic/versions/`
- Updated `models/database.py`
- New `models/schemas.py` with Book* models
- `repositories/book_repository.py`
- `repositories/book_guideline_repository.py`

---

### Phase 2: AWS S3 Infrastructure
**Duration:** 1 day
**Blockers:** None (AWS credentials in ~/.aws/credentials)

**Tasks:**
1. Add `boto3` and `Pillow` to `requirements.txt`
2. Add AWS config to `config.py` (region, bucket name - credentials auto-detected)
3. Create `utils/s3_client.py` with S3 operations:
   - `upload_file(local_path, s3_key)` → S3 URL
   - `download_file(s3_key, local_path)` → file
   - `get_file_url(s3_key)` → presigned URL
   - `delete_file(s3_key)` → bool
   - `update_metadata_json(book_id, metadata)` → bool
4. Create S3 bucket `learnlikemagic-books` (if doesn't exist)
5. Set up folder structure: `books/{book_id}/`

**Deliverables:**
- `utils/s3_client.py` with full CRUD operations
- Updated `config.py` with AWS settings
- S3 bucket created and verified

---

### Phase 3: Book Management Backend APIs
**Duration:** 2-3 days
**Dependencies:** Phase 1 complete
**Blockers:** None (can use local filesystem for testing if S3 not ready)

**Tasks:**
1. Create `services/book_service.py` with business logic
2. Create `api/routes/admin.py` with book CRUD endpoints
3. Implement book status state machine transitions
4. Add book metadata extraction placeholder (will integrate Vision API in Phase 4)

**API Endpoints:**

```
POST   /admin/books                    - Create new book
GET    /admin/books                    - List all books (with filters)
GET    /admin/books/{book_id}          - Get book details + pages
PUT    /admin/books/{book_id}          - Update book metadata
DELETE /admin/books/{book_id}          - Delete book + S3 files
PUT    /admin/books/{book_id}/status   - Update book status
```

**Request/Response Models:**

```python
# Create Book Request
{
  "title": "Math Magic",
  "author": "NCERT",
  "edition": "2024",
  "edition_year": 2024,
  "country": "India",
  "board": "CBSE",
  "grade": 3,
  "subject": "Mathematics"
}

# Book Response
{
  "id": "ncert_math_3_2024",
  "title": "Math Magic",
  "author": "NCERT",
  "status": "draft",
  "pages": [],
  "s3_prefix": "books/ncert_math_3_2024/",
  "created_at": "2025-10-25T10:00:00Z",
  "metadata": { ... }
}
```

**Deliverables:**
- `services/book_service.py`
- `api/routes/admin.py`
- Unit tests in `tests/unit/test_book_service.py`
- Integration tests in `tests/integration/test_book_api.py`

---

### Phase 4: OCR & Page Upload
**Duration:** 2-3 days
**Dependencies:** Phase 2, Phase 3
**Blockers:** None (Vision API reference code received)

**Tasks:**
1. Create `services/ocr_service.py` with Vision API integration
2. Add page upload endpoints to `api/routes/admin.py`
3. Implement image validation (format, size)
4. Store images and OCR text in S3
5. Update `metadata.json` in S3 after each approval

**API Endpoints:**

```
POST   /admin/books/{book_id}/pages                    - Upload page image
POST   /admin/books/{book_id}/pages/{page_num}/ocr     - Trigger OCR (may be auto)
PUT    /admin/books/{book_id}/pages/{page_num}/approve - Approve page
DELETE /admin/books/{book_id}/pages/{page_num}         - Reject/delete page (allows re-upload)
GET    /admin/books/{book_id}/pages                    - List all pages with status
GET    /admin/books/{book_id}/pages/{page_num}         - Get page details + presigned URLs
```

**Page Upload Flow:**

```python
1. POST /admin/books/{book_id}/pages
   - Request: multipart/form-data with image file
   - Validate: format (PNG/JPG/TIFF), size (<10MB)
   - Generate page_num (next available)
   - Store: s3://learnlikemagic-books/books/{book_id}/{page_num}.png
   - Auto-trigger OCR via Vision API
   - Store OCR text: books/{book_id}/{page_num}.txt
   - Return: page_num, image_url, ocr_text, status: "pending_review"

2. PUT /admin/books/{book_id}/pages/{page_num}/approve
   - Update metadata.json in S3 with approved status
   - Update book status to "uploading_pages" if first page
   - Return: success

3. DELETE /admin/books/{book_id}/pages/{page_num}
   - Delete image and text from S3
   - Remove from metadata.json
   - Allow re-upload at same page_num
```

**metadata.json Structure:**

```json
{
  "book_id": "ncert_math_3_2024",
  "pages": [
    {
      "page_num": 1,
      "image_s3_key": "books/ncert_math_3_2024/1.png",
      "text_s3_key": "books/ncert_math_3_2024/1.txt",
      "status": "approved",
      "approved_at": "2025-10-25T10:30:00Z"
    },
    {
      "page_num": 2,
      "image_s3_key": "books/ncert_math_3_2024/2.png",
      "text_s3_key": "books/ncert_math_3_2024/2.txt",
      "status": "approved",
      "approved_at": "2025-10-25T10:35:00Z"
    }
  ],
  "total_pages": 2,
  "last_updated": "2025-10-25T10:35:00Z"
}
```

**OCR Service:**

```python
# services/ocr_service.py
import base64
from pathlib import Path
from openai import OpenAI
from config import get_settings

class OCRService:
    def __init__(self):
        settings = get_settings()
        self.client = OpenAI(api_key=settings.openai_api_key)
        self.model = "gpt-4o-mini"
        self.max_tokens = 4096

    def encode_image_to_base64(self, image_path: str) -> str:
        """Encode image to base64."""
        with open(image_path, 'rb') as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    def extract_text_from_image(self, image_path: str) -> str:
        """
        Use OpenAI Vision API to extract complete text from textbook page.

        Extracts all text, drawings, images, formulas, captions, and visual elements.

        Args:
            image_path: Path to the book page image

        Returns:
            Complete interpretation of the book page as text
        """
        # Encode the image
        base64_image = self.encode_image_to_base64(image_path)

        # Detailed interpretation prompt
        prompt_text = """I have a book page image that I need you to interpret completely.

Please read this image and give me each and everything that's present in this book page image.
Give the complete interpretation in form of text - even the drawings and any images in this book page.

Please provide:
- All titles and headings
- All body text and paragraphs
- Descriptions of all images, diagrams, and illustrations
- Labels and captions
- Any equations, formulas, or special formatting
- Page numbers and footers
- Any other visual elements

Format the output clearly with appropriate structure and formatting."""

        # Create the API request
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt_text},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{base64_image}"
                        }
                    },
                ],
            }],
            max_tokens=self.max_tokens
        )

        return response.choices[0].message.content
```

**Deliverables:**
- `services/ocr_service.py`
- Updated `api/routes/admin.py` with page endpoints
- Image upload handling with validation
- S3 metadata.json management
- Tests for OCR service

---

### Phase 5: Admin Frontend UI
**Duration:** 3-4 days
**Dependencies:** Phase 3, Phase 4
**Blockers:** None (can mock API responses)

**Tasks:**
1. Add admin routes to `llm-frontend/src/App.tsx`
2. Create admin page components
3. Implement hardcoded auth (admin/admin)
4. Build book management dashboard
5. Build page upload interface with side-by-side review
6. Implement approved pages sidebar
7. Add file upload with preview

**Component Structure:**

```
llm-frontend/src/
├── pages/
│   └── admin/
│       ├── AdminLogin.tsx           - Login form (admin/admin)
│       ├── BooksDashboard.tsx       - List books, create new
│       ├── CreateBook.tsx           - New book form
│       ├── BookDetail.tsx           - Main book management page
│       │   ├── PageUploadPanel.tsx  - Left: upload + review
│       │   ├── PagesSidebar.tsx     - Right: approved pages list
│       │   └── GuidelinesPanel.tsx  - Bottom: guideline review (Phase 6)
│       └── components/
│           ├── BookCard.tsx         - Book list item
│           ├── BookStatusBadge.tsx  - Status indicator
│           ├── ImageTextViewer.tsx  - Side-by-side image/text
│           └── FileUploader.tsx     - Drag-n-drop uploader
├── api/
│   └── adminApi.ts                  - Admin API client
└── hooks/
    └── useAdminAuth.ts              - Simple auth hook
```

**Key UI Screens:**

1. **Login Page (`/admin`)**
   ```tsx
   - Username input (default: admin)
   - Password input (default: admin)
   - Login button
   - On success: store token in localStorage, redirect to /admin/books
   ```

2. **Books Dashboard (`/admin/books`)**
   ```tsx
   - Header: "Books" + "Create New Book" button
   - Filter bar: Board, Grade, Subject, Status
   - Book cards grid:
     - Cover image thumbnail
     - Title, Author, Grade, Subject
     - Status badge
     - Page count
     - Click to open detail
   ```

3. **Create Book (`/admin/books/new`)**
   ```tsx
   - Form fields: Title, Author, Edition, Year, Country, Board, Grade, Subject
   - Cover image upload (optional for MVP)
   - Submit → POST /admin/books → redirect to book detail
   ```

4. **Book Detail (`/admin/books/{id}`)**
   ```tsx
   - Header:
     - Book metadata (title, author, etc.)
     - Status badge (with transitions)
     - Actions: "Mark Complete", "Generate Guidelines"

   - Layout: 3-column

     [Left Panel: Page Upload (40%)]
     - If status = "draft" or "uploading_pages":
       - File upload dropzone
       - "Upload Page" button
       - After upload:
         - Image preview (top)
         - OCR text (bottom)
         - Approve / Reject buttons

     [Middle Panel: Page Viewer (30%)]
     - Selected page display (from sidebar)
     - Image + text in tabs or split view

     [Right Sidebar: Approved Pages (30%)]
     - Scrollable list of approved pages
     - Each item: thumbnail + page number
     - Click to view in middle panel

   - Bottom Panel: Guidelines (appears when status = "guidelines_pending_review")
     - JSON viewer with syntax highlighting
     - Approve / Reject buttons
   ```

**State Management:**

```tsx
// BookDetail.tsx
const [book, setBook] = useState<Book | null>(null);
const [currentPage, setCurrentPage] = useState<number | null>(null);
const [uploadedImage, setUploadedImage] = useState<File | null>(null);
const [ocrText, setOcrText] = useState<string>("");
const [isProcessing, setIsProcessing] = useState(false);

// Upload flow
const handleImageUpload = async (file: File) => {
  setUploadedImage(file);
  setIsProcessing(true);

  const formData = new FormData();
  formData.append('image', file);

  const result = await adminApi.uploadPage(bookId, formData);
  setCurrentPage(result.page_num);
  setOcrText(result.ocr_text);
  setIsProcessing(false);
};

const handleApprovePage = async () => {
  await adminApi.approvePage(bookId, currentPage);
  // Refresh book data
  fetchBook();
  // Reset upload form
  setUploadedImage(null);
  setOcrText("");
};
```

**Deliverables:**
- Complete admin UI with all screens
- API integration
- File upload with preview
- Side-by-side review interface
- Responsive design (desktop-first for MVP)

---

### Phase 6: Guideline Generation (LangGraph Workflow)
**Duration:** 3-4 days
**Dependencies:** Phase 4 (need approved pages)
**Blockers:** None

**Tasks:**
1. Design LangGraph state schema for guideline extraction
2. Create graph nodes for each extraction step
3. Create prompts for each node
4. Implement incremental page processing with context
5. Add guideline generation endpoint
6. Implement guideline approval endpoint with auto-population logic

**LangGraph Architecture:**

```
graph/
├── guideline_extraction/
│   ├── state.py                 - GuidelineState definition
│   ├── nodes.py                 - All node implementations
│   ├── build_graph.py           - Graph compilation
│   └── utils.py                 - Helper functions
└── prompts/
    └── guideline_extraction/
        ├── extract_topics.txt
        ├── extract_subtopics.txt
        ├── extract_objectives.txt
        ├── identify_misconceptions.txt
        └── synthesize_guideline.txt
```

**State Schema:**

```python
# graph/guideline_extraction/state.py
from typing import TypedDict, List, Dict, Any

class GuidelineState(TypedDict):
    book_id: str
    book_metadata: Dict[str, Any]
    pages: List[Dict[str, Any]]  # [{"page_num": 1, "text": "..."}]
    current_page_idx: int

    # Accumulated extractions
    topics: List[Dict[str, Any]]  # [{"topic": "Fractions", "subtopics": [...]}]
    current_topic: str
    context_from_previous_page: str

    # Final output
    guideline_json: Dict[str, Any]
    error: str | None
```

**Graph Nodes:**

```python
# graph/guideline_extraction/nodes.py

def extract_topics_node(state: GuidelineState) -> GuidelineState:
    """
    Analyze all page texts to identify main topics/chapters.

    Input: All page texts
    Output: List of topic names
    """
    all_text = "\n\n".join([p["text"] for p in state["pages"]])

    prompt = load_prompt("extract_topics")
    response = llm.invoke(prompt.format(
        book_metadata=state["book_metadata"],
        all_text=all_text
    ))

    topics = parse_topics_from_response(response)
    state["topics"] = topics
    return state


def extract_subtopics_node(state: GuidelineState) -> GuidelineState:
    """
    For each topic, extract subtopics incrementally page-by-page.

    Uses context from previous pages to detect:
    - Topic continuation
    - New subtopic start
    - Topic end
    """
    # Process pages sequentially
    for page_idx, page in enumerate(state["pages"]):
        context = state.get("context_from_previous_page", "")

        prompt = load_prompt("extract_subtopics")
        response = llm.invoke(prompt.format(
            current_page_text=page["text"],
            previous_context=context,
            identified_topics=state["topics"],
            page_number=page["page_num"]
        ))

        # Update topics with new subtopics
        updated_topics = parse_subtopics_from_response(response)
        state["topics"] = merge_topics(state["topics"], updated_topics)

        # Store context for next page
        state["context_from_previous_page"] = generate_context_summary(
            page["text"],
            updated_topics
        )

    return state


def extract_learning_objectives_node(state: GuidelineState) -> GuidelineState:
    """
    For each subtopic, determine learning objectives.
    """
    for topic in state["topics"]:
        for subtopic in topic["subtopics"]:
            # Get relevant page texts for this subtopic
            relevant_text = get_text_for_pages(
                state["pages"],
                subtopic["source_pages"]
            )

            prompt = load_prompt("extract_objectives")
            response = llm.invoke(prompt.format(
                topic=topic["topic"],
                subtopic=subtopic["subtopic"],
                content=relevant_text,
                grade=state["book_metadata"]["grade"]
            ))

            objectives = parse_objectives_from_response(response)
            subtopic["metadata"]["learning_objectives"] = objectives

    return state


def identify_misconceptions_node(state: GuidelineState) -> GuidelineState:
    """
    Identify common misconceptions for each subtopic.
    """
    for topic in state["topics"]:
        for subtopic in topic["subtopics"]:
            relevant_text = get_text_for_pages(
                state["pages"],
                subtopic["source_pages"]
            )

            prompt = load_prompt("identify_misconceptions")
            response = llm.invoke(prompt.format(
                topic=topic["topic"],
                subtopic=subtopic["subtopic"],
                content=relevant_text
            ))

            misconceptions = parse_misconceptions_from_response(response)
            subtopic["metadata"]["common_misconceptions"] = misconceptions

    return state


def extract_assessment_criteria_node(state: GuidelineState) -> GuidelineState:
    """
    Generate assessment criteria and example problems.
    """
    # Similar pattern to above nodes
    pass


def synthesize_guideline_node(state: GuidelineState) -> GuidelineState:
    """
    Combine all extracted data into final guideline.json structure.
    """
    guideline_json = {
        "book_id": state["book_id"],
        "book_metadata": state["book_metadata"],
        "topics": state["topics"]
    }

    state["guideline_json"] = guideline_json
    return state
```

**Graph Compilation:**

```python
# graph/guideline_extraction/build_graph.py
from langgraph.graph import StateGraph, END

def build_guideline_extraction_graph():
    workflow = StateGraph(GuidelineState)

    # Add nodes
    workflow.add_node("extract_topics", extract_topics_node)
    workflow.add_node("extract_subtopics", extract_subtopics_node)
    workflow.add_node("extract_objectives", extract_learning_objectives_node)
    workflow.add_node("identify_misconceptions", identify_misconceptions_node)
    workflow.add_node("extract_assessment", extract_assessment_criteria_node)
    workflow.add_node("synthesize", synthesize_guideline_node)

    # Define flow
    workflow.set_entry_point("extract_topics")
    workflow.add_edge("extract_topics", "extract_subtopics")
    workflow.add_edge("extract_subtopics", "extract_objectives")
    workflow.add_edge("extract_objectives", "identify_misconceptions")
    workflow.add_edge("identify_misconceptions", "extract_assessment")
    workflow.add_edge("extract_assessment", "synthesize")
    workflow.add_edge("synthesize", END)

    return workflow.compile()
```

**API Endpoints:**

```
POST   /admin/books/{book_id}/generate-guidelines   - Trigger guideline generation
GET    /admin/books/{book_id}/guidelines            - Get generated guideline.json
PUT    /admin/books/{book_id}/guidelines/approve    - Approve & populate teaching_guidelines
PUT    /admin/books/{book_id}/guidelines/reject     - Reject & allow retry
```

**Guideline Approval Logic:**

```python
# services/book_service.py
def approve_guideline(book_id: str) -> bool:
    """
    Approve guideline and populate teaching_guidelines table.
    """
    # 1. Load guideline.json from S3
    guideline_json = s3_client.get_json(f"books/{book_id}/guideline.json")

    # 2. Parse and create teaching_guideline rows
    book = book_repo.get_by_id(book_id)

    for topic in guideline_json["topics"]:
        for subtopic_data in topic["subtopics"]:
            teaching_guideline = TeachingGuideline(
                id=generate_id(),
                country=book.country,
                board=book.board,
                grade=book.grade,
                subject=book.subject,
                topic=topic["topic"],
                subtopic=subtopic_data["subtopic"],
                guideline=subtopic_data["guideline"],
                metadata_json=json.dumps(subtopic_data["metadata"]),
                book_id=book_id,
                source_pages=json.dumps(subtopic_data["source_pages"])
            )
            guideline_repo.create(teaching_guideline)

    # 3. Update book status
    book_repo.update_status(book_id, "approved")

    # 4. Update book_guidelines status
    book_guideline_repo.update_status(book_id, "approved")

    return True
```

**Deliverables:**
- Complete LangGraph workflow
- All node implementations with prompts
- Guideline generation API endpoint
- Approval logic with auto-population
- Tests for graph execution
- Integration test with sample book

---

### Phase 7: Testing & Integration
**Duration:** 2-3 days
**Dependencies:** All previous phases

**Tasks:**
1. End-to-end testing with NCERT Math Magic Grade 3
2. Unit tests for all services and repositories
3. Integration tests for API endpoints
4. Manual UI testing
5. Performance testing (OCR speed, S3 upload time)
6. Bug fixes and refinements

**Test Coverage:**

```
Unit Tests:
- book_service.py: CRUD operations, status transitions
- ocr_service.py: Mock Vision API calls
- s3_client.py: Mock boto3 operations
- book_repository.py: Database operations

Integration Tests:
- POST /admin/books → verify DB insert + S3 folder creation
- POST /admin/books/{id}/pages → verify OCR + S3 storage
- PUT /admin/books/{id}/pages/{num}/approve → verify metadata update
- POST /admin/books/{id}/generate-guidelines → verify LangGraph execution
- PUT /admin/books/{id}/guidelines/approve → verify teaching_guidelines population

E2E Test:
- Full workflow: Create book → Upload 5 pages → Approve all → Generate guidelines → Approve → Verify AI tutor can query
```

**Deliverables:**
- 80%+ test coverage
- E2E test passing with sample book
- Performance benchmarks documented
- Bug fixes applied
- User acceptance testing completed

---

## 3. Database Schema (SQL)

```sql
-- New table: books
CREATE TABLE books (
    id VARCHAR PRIMARY KEY,
    title VARCHAR NOT NULL,
    author VARCHAR,
    edition VARCHAR,
    edition_year INTEGER,
    country VARCHAR NOT NULL,
    board VARCHAR NOT NULL,
    grade INTEGER NOT NULL,
    subject VARCHAR NOT NULL,
    cover_image_s3_key VARCHAR,
    s3_prefix VARCHAR NOT NULL,
    status VARCHAR NOT NULL CHECK (status IN (
        'draft',
        'uploading_pages',
        'pages_complete',
        'generating_guidelines',
        'guidelines_pending_review',
        'approved'
    )),
    metadata_s3_key VARCHAR,  -- books/{book_id}/metadata.json
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    created_by VARCHAR DEFAULT 'admin'
);

CREATE INDEX idx_books_curriculum ON books(country, board, grade, subject);
CREATE INDEX idx_books_status ON books(status);


-- New table: book_guidelines
CREATE TABLE book_guidelines (
    id VARCHAR PRIMARY KEY,
    book_id VARCHAR NOT NULL REFERENCES books(id) ON DELETE CASCADE,
    guideline_s3_key VARCHAR NOT NULL,  -- books/{book_id}/guideline.json
    status VARCHAR NOT NULL CHECK (status IN (
        'draft',
        'pending_review',
        'approved',
        'rejected'
    )),
    generated_at TIMESTAMP,
    reviewed_at TIMESTAMP,
    reviewed_by VARCHAR,
    version INTEGER DEFAULT 1,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_book_guidelines_book ON book_guidelines(book_id);


-- Modify existing table: teaching_guidelines
ALTER TABLE teaching_guidelines
ADD COLUMN book_id VARCHAR REFERENCES books(id) ON DELETE SET NULL,
ADD COLUMN source_pages VARCHAR;  -- JSON array: "[15, 16, 17]"

CREATE INDEX idx_teaching_guidelines_book ON teaching_guidelines(book_id);
```

**Alembic Migration:**

```python
# alembic/versions/001_add_book_ingestion_tables.py
def upgrade():
    # Create books table
    op.create_table(
        'books',
        sa.Column('id', sa.String(), nullable=False),
        sa.Column('title', sa.String(), nullable=False),
        sa.Column('author', sa.String(), nullable=True),
        sa.Column('edition', sa.String(), nullable=True),
        sa.Column('edition_year', sa.Integer(), nullable=True),
        sa.Column('country', sa.String(), nullable=False),
        sa.Column('board', sa.String(), nullable=False),
        sa.Column('grade', sa.Integer(), nullable=False),
        sa.Column('subject', sa.String(), nullable=False),
        sa.Column('cover_image_s3_key', sa.String(), nullable=True),
        sa.Column('s3_prefix', sa.String(), nullable=False),
        sa.Column('status', sa.String(), nullable=False),
        sa.Column('metadata_s3_key', sa.String(), nullable=True),
        sa.Column('created_at', sa.DateTime(), server_default=sa.func.now()),
        sa.Column('updated_at', sa.DateTime(), server_default=sa.func.now()),
        sa.Column('created_by', sa.String(), server_default='admin'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_books_curriculum', 'books', ['country', 'board', 'grade', 'subject'])
    op.create_index('idx_books_status', 'books', ['status'])

    # Create book_guidelines table
    op.create_table(
        'book_guidelines',
        sa.Column('id', sa.String(), nullable=False),
        sa.Column('book_id', sa.String(), nullable=False),
        sa.Column('guideline_s3_key', sa.String(), nullable=False),
        sa.Column('status', sa.String(), nullable=False),
        sa.Column('generated_at', sa.DateTime(), nullable=True),
        sa.Column('reviewed_at', sa.DateTime(), nullable=True),
        sa.Column('reviewed_by', sa.String(), nullable=True),
        sa.Column('version', sa.Integer(), server_default='1'),
        sa.Column('created_at', sa.DateTime(), server_default=sa.func.now()),
        sa.PrimaryKeyConstraint('id'),
        sa.ForeignKeyConstraint(['book_id'], ['books.id'], ondelete='CASCADE')
    )
    op.create_index('idx_book_guidelines_book', 'book_guidelines', ['book_id'])

    # Modify teaching_guidelines table
    op.add_column('teaching_guidelines',
                  sa.Column('book_id', sa.String(), nullable=True))
    op.add_column('teaching_guidelines',
                  sa.Column('source_pages', sa.String(), nullable=True))
    op.create_foreign_key(
        'fk_teaching_guidelines_book',
        'teaching_guidelines',
        'books',
        ['book_id'],
        ['id'],
        ondelete='SET NULL'
    )
    op.create_index('idx_teaching_guidelines_book',
                    'teaching_guidelines',
                    ['book_id'])

def downgrade():
    # Reverse all changes
    op.drop_index('idx_teaching_guidelines_book', 'teaching_guidelines')
    op.drop_constraint('fk_teaching_guidelines_book', 'teaching_guidelines')
    op.drop_column('teaching_guidelines', 'source_pages')
    op.drop_column('teaching_guidelines', 'book_id')

    op.drop_index('idx_book_guidelines_book', 'book_guidelines')
    op.drop_table('book_guidelines')

    op.drop_index('idx_books_status', 'books')
    op.drop_index('idx_books_curriculum', 'books')
    op.drop_table('books')
```

---

## 4. API Specification

### 4.1 Admin Authentication
```
POST /admin/login
Request:
{
  "username": "admin",
  "password": "admin"
}

Response:
{
  "token": "simple-jwt-token",
  "username": "admin"
}
```

### 4.2 Book Management
```
POST /admin/books
Request:
{
  "title": "Math Magic",
  "author": "NCERT",
  "edition": "2024",
  "edition_year": 2024,
  "country": "India",
  "board": "CBSE",
  "grade": 3,
  "subject": "Mathematics"
}

Response: 201 Created
{
  "id": "ncert_math_3_2024",
  "title": "Math Magic",
  "status": "draft",
  "s3_prefix": "books/ncert_math_3_2024/",
  "created_at": "2025-10-25T10:00:00Z"
}

---

GET /admin/books?country=India&board=CBSE&grade=3

Response: 200 OK
{
  "books": [
    {
      "id": "ncert_math_3_2024",
      "title": "Math Magic",
      "author": "NCERT",
      "status": "uploading_pages",
      "page_count": 5,
      "created_at": "2025-10-25T10:00:00Z"
    }
  ],
  "total": 1
}

---

GET /admin/books/{book_id}

Response: 200 OK
{
  "id": "ncert_math_3_2024",
  "title": "Math Magic",
  "author": "NCERT",
  "status": "uploading_pages",
  "pages": [
    {
      "page_num": 1,
      "status": "approved",
      "image_url": "https://s3.../1.png",
      "text_url": "https://s3.../1.txt"
    }
  ],
  "metadata": { ... }
}

---

PUT /admin/books/{book_id}/status
Request:
{
  "status": "pages_complete"
}

Response: 200 OK
{
  "id": "ncert_math_3_2024",
  "status": "pages_complete"
}
```

### 4.3 Page Management
```
POST /admin/books/{book_id}/pages
Request: multipart/form-data
- image: File (PNG/JPG/TIFF)

Response: 201 Created
{
  "page_num": 1,
  "image_url": "https://s3-presigned-url.../1.png",
  "ocr_text": "Chapter 1: Fractions\n\nA fraction represents...",
  "status": "pending_review"
}

---

PUT /admin/books/{book_id}/pages/{page_num}/approve

Response: 200 OK
{
  "page_num": 1,
  "status": "approved"
}

---

DELETE /admin/books/{book_id}/pages/{page_num}

Response: 204 No Content
```

### 4.4 Guideline Generation
```
POST /admin/books/{book_id}/generate-guidelines

Response: 202 Accepted
{
  "message": "Guideline generation started",
  "estimated_time": "2-3 minutes"
}

---

GET /admin/books/{book_id}/guidelines

Response: 200 OK
{
  "book_id": "ncert_math_3_2024",
  "status": "pending_review",
  "guideline": {
    "book_id": "ncert_math_3_2024",
    "book_metadata": { ... },
    "topics": [ ... ]
  },
  "generated_at": "2025-10-25T11:00:00Z"
}

---

PUT /admin/books/{book_id}/guidelines/approve

Response: 200 OK
{
  "message": "Guideline approved and populated to teaching_guidelines",
  "teaching_guidelines_created": 6
}

---

PUT /admin/books/{book_id}/guidelines/reject
Request:
{
  "reason": "Missing subtopics for chapter 3"
}

Response: 200 OK
{
  "message": "Guideline rejected, can be regenerated"
}
```

---

## 5. LangGraph Workflow Details

### 5.1 Graph Visualization

```
                 ┌─────────────────────┐
                 │  START              │
                 └──────────┬──────────┘
                            │
                            ▼
                 ┌─────────────────────┐
                 │  extract_topics     │
                 │  (analyze all pages)│
                 └──────────┬──────────┘
                            │
                            ▼
                 ┌─────────────────────┐
                 │ extract_subtopics   │
                 │ (incremental, page  │
                 │  by page with ctx)  │
                 └──────────┬──────────┘
                            │
                            ▼
                 ┌─────────────────────┐
                 │ extract_objectives  │
                 │ (for each subtopic) │
                 └──────────┬──────────┘
                            │
                            ▼
                 ┌─────────────────────┐
                 │identify_misconceptions│
                 │ (per subtopic)      │
                 └──────────┬──────────┘
                            │
                            ▼
                 ┌─────────────────────┐
                 │extract_assessment   │
                 │ (criteria + examples)│
                 └──────────┬──────────┘
                            │
                            ▼
                 ┌─────────────────────┐
                 │  synthesize         │
                 │  (combine into JSON)│
                 └──────────┬──────────┘
                            │
                            ▼
                 ┌─────────────────────┐
                 │   END               │
                 └─────────────────────┘
```

### 5.2 Prompt Examples

**extract_subtopics.txt:**
```
You are an expert curriculum analyzer for grade {grade} {subject}.

TASK: Identify subtopics from this textbook page.

CONTEXT FROM PREVIOUS PAGE:
{previous_context}

CURRENT PAGE (Page {page_number}):
{current_page_text}

IDENTIFIED TOPICS SO FAR:
{identified_topics}

Analyze this page and determine:
1. Is this a continuation of a previous subtopic, or a new subtopic?
2. What is the subtopic name?
3. Which topic does this subtopic belong to?
4. What page numbers does this subtopic span?

Return your analysis in this JSON format:
{{
  "is_continuation": true/false,
  "subtopic": "Subtopic name",
  "topic": "Parent topic",
  "page_range_start": {page_number},
  "page_range_end": {page_number},  // Will be updated as we see more pages
  "key_concepts": ["concept1", "concept2"]
}}

IMPORTANT: Use the context from the previous page to make accurate decisions about topic boundaries.
```

---

## 6. Frontend Component Details

### 6.1 BookDetail.tsx (Main Component)

```tsx
// llm-frontend/src/pages/admin/BookDetail.tsx
import { useState, useEffect } from 'react';
import { useParams } from 'react-router-dom';
import { adminApi } from '../../api/adminApi';
import PageUploadPanel from './components/PageUploadPanel';
import PagesSidebar from './components/PagesSidebar';
import GuidelinesPanel from './components/GuidelinesPanel';

export default function BookDetail() {
  const { bookId } = useParams();
  const [book, setBook] = useState(null);
  const [selectedPage, setSelectedPage] = useState(null);

  useEffect(() => {
    loadBook();
  }, [bookId]);

  const loadBook = async () => {
    const data = await adminApi.getBook(bookId);
    setBook(data);
  };

  const handlePageApproved = () => {
    loadBook(); // Refresh to show new page in sidebar
  };

  const handleMarkComplete = async () => {
    await adminApi.updateBookStatus(bookId, 'pages_complete');
    loadBook();
  };

  const handleGenerateGuidelines = async () => {
    await adminApi.generateGuidelines(bookId);
    loadBook();
  };

  return (
    <div className="book-detail">
      {/* Header */}
      <div className="book-header">
        <h1>{book?.title}</h1>
        <StatusBadge status={book?.status} />
        <div className="actions">
          {book?.status === 'uploading_pages' && (
            <button onClick={handleMarkComplete}>
              Mark Book Complete
            </button>
          )}
          {book?.status === 'pages_complete' && (
            <button onClick={handleGenerateGuidelines}>
              Generate Guidelines
            </button>
          )}
        </div>
      </div>

      {/* Main Content */}
      <div className="book-content">
        {/* Left: Page Upload */}
        <div className="left-panel">
          <PageUploadPanel
            bookId={bookId}
            onPageApproved={handlePageApproved}
          />
        </div>

        {/* Right: Approved Pages */}
        <div className="right-sidebar">
          <PagesSidebar
            pages={book?.pages || []}
            selectedPage={selectedPage}
            onSelectPage={setSelectedPage}
          />
        </div>
      </div>

      {/* Bottom: Guidelines (conditional) */}
      {book?.status === 'guidelines_pending_review' && (
        <GuidelinesPanel
          bookId={bookId}
          onApprove={loadBook}
          onReject={loadBook}
        />
      )}
    </div>
  );
}
```

### 6.2 PageUploadPanel.tsx

```tsx
// llm-frontend/src/pages/admin/components/PageUploadPanel.tsx
import { useState } from 'react';
import { adminApi } from '../../../api/adminApi';
import ImageTextViewer from './ImageTextViewer';

export default function PageUploadPanel({ bookId, onPageApproved }) {
  const [uploadedImage, setUploadedImage] = useState(null);
  const [imagePreview, setImagePreview] = useState(null);
  const [ocrText, setOcrText] = useState('');
  const [pageNum, setPageNum] = useState(null);
  const [isProcessing, setIsProcessing] = useState(false);

  const handleFileSelect = async (e) => {
    const file = e.target.files[0];
    if (!file) return;

    // Show preview
    setUploadedImage(file);
    setImagePreview(URL.createObjectURL(file));

    // Upload and OCR
    setIsProcessing(true);
    try {
      const formData = new FormData();
      formData.append('image', file);

      const result = await adminApi.uploadPage(bookId, formData);
      setPageNum(result.page_num);
      setOcrText(result.ocr_text);
    } catch (error) {
      console.error('Upload failed:', error);
    } finally {
      setIsProcessing(false);
    }
  };

  const handleApprove = async () => {
    await adminApi.approvePage(bookId, pageNum);
    // Reset form
    setUploadedImage(null);
    setImagePreview(null);
    setOcrText('');
    setPageNum(null);
    onPageApproved();
  };

  const handleReject = async () => {
    await adminApi.deletePage(bookId, pageNum);
    // Reset form to allow re-upload
    setUploadedImage(null);
    setImagePreview(null);
    setOcrText('');
    setPageNum(null);
  };

  return (
    <div className="page-upload-panel">
      <h2>Upload Page</h2>

      {!uploadedImage ? (
        <div className="upload-dropzone">
          <input
            type="file"
            accept="image/*"
            onChange={handleFileSelect}
          />
          <p>Click or drag to upload page image</p>
        </div>
      ) : (
        <>
          {isProcessing ? (
            <div className="processing">
              <p>Processing OCR...</p>
            </div>
          ) : (
            <>
              <ImageTextViewer
                imageUrl={imagePreview}
                text={ocrText}
              />

              <div className="actions">
                <button onClick={handleApprove} className="approve">
                  Approve
                </button>
                <button onClick={handleReject} className="reject">
                  Reject & Re-upload
                </button>
              </div>
            </>
          )}
        </>
      )}
    </div>
  );
}
```

---

## 7. Dependencies & Blockers

### 7.1 Current Blockers

**ALL BLOCKERS RESOLVED ✅**

1. **AWS Credentials** ✅ RESOLVED
   - **Status:** Configured in `~/.aws/credentials`
   - **Region:** `us-east-1`
   - **boto3 auto-detection:** Credentials automatically picked up from default AWS credential chain
   - **Action:** None needed - Terraform already successfully using these credentials

2. **OpenAI Vision API Reference Code** ✅ RESOLVED
   - **Status:** Reference implementation received
   - **Model:** `gpt-4o-mini` with 4096 max tokens
   - **Implementation:** Complete OCR service code defined in Phase 4
   - **Action:** Ready to implement

### 7.2 New Dependencies

**Backend (`requirements.txt`):**
```
boto3==1.34.0              # AWS S3 SDK
Pillow==10.1.0             # Image processing
python-multipart==0.0.6    # File upload support
```

**Frontend (`package.json`):**
```json
{
  "dependencies": {
    "react-router-dom": "^6.20.0",  // Already exists
    // No new deps needed - using built-in File API
  }
}
```

---

## 8. Timeline Estimates

**Assuming AWS credentials and Vision API reference provided:**

| Phase | Duration | Dependencies | Start Date | End Date |
|-------|----------|--------------|------------|----------|
| Phase 1: Database | 1-2 days | None | Day 1 | Day 2 |
| Phase 2: AWS S3 | 1 day | Phase 1, AWS creds | Day 3 | Day 3 |
| Phase 3: Book APIs | 2-3 days | Phase 1 | Day 2 | Day 4 |
| Phase 4: OCR | 2-3 days | Phase 2, 3, Vision API | Day 4 | Day 6 |
| Phase 5: Frontend | 3-4 days | Phase 3, 4 | Day 5 | Day 8 |
| Phase 6: LangGraph | 3-4 days | Phase 4 | Day 7 | Day 10 |
| Phase 7: Testing | 2-3 days | All phases | Day 11 | Day 13 |

**Total Estimated Duration:** 13-15 working days (~3 weeks)

**Can Start Immediately:**
- Phase 1 (Database) - No blockers
- Phase 3 (Book APIs) - Can mock S3 calls initially

**Critical Path:**
Phase 1 → Phase 2 → Phase 4 → Phase 6 → Phase 7

---

## 9. Testing Strategy

### 9.1 Unit Tests
- All services: 100% coverage
- All repositories: 100% coverage
- Mock external dependencies (S3, OpenAI)

### 9.2 Integration Tests
- API endpoints with real database
- S3 operations with mocked boto3
- LangGraph execution with mocked LLM

### 9.3 E2E Test
**Scenario:** Upload NCERT Math Magic Grade 3

1. Create book via admin UI
2. Upload 10 sample pages
3. Approve all pages
4. Generate guidelines
5. Approve guidelines
6. Verify teaching_guidelines table populated
7. Verify AI tutor can query new guidelines
8. Create teaching session with new guideline
9. Verify tutor can teach using book content

### 9.4 Performance Benchmarks
- Page upload + OCR: < 10 seconds
- Guideline generation (50 pages): < 5 minutes
- S3 upload (1MB image): < 2 seconds

---

## 10. Deployment Considerations

### 10.1 Environment Variables
Add to `.env` (AWS credentials auto-detected from `~/.aws/credentials`):
```
# AWS Configuration (credentials in ~/.aws/credentials)
AWS_REGION=us-east-1
AWS_S3_BUCKET=learnlikemagic-books
```

### 10.2 Database Migration
Run Alembic migration before deploying:
```bash
cd llm-backend
alembic upgrade head
```

### 10.3 S3 Bucket Setup
- Create bucket: `learnlikemagic-books`
- Enable versioning (optional)
- Set CORS policy for frontend uploads
- Configure lifecycle rules (optional, for cleanup)

### 10.4 IAM Permissions
For production, use IAM role with these permissions:
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObject",
        "s3:DeleteObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::learnlikemagic-books",
        "arn:aws:s3:::learnlikemagic-books/*"
      ]
    }
  ]
}
```

---

## 11. Risks & Mitigations

### 11.1 OCR Accuracy
**Risk:** OpenAI Vision API may not perfectly extract mathematical notation
**Mitigation:**
- Allow admin to reject and re-upload
- Post-MVP: Add inline editing
- Consider hybrid approach with specialized math OCR

### 11.2 Guideline Quality
**Risk:** AI-generated guidelines may be incomplete or inaccurate
**Mitigation:**
- Admin review required before approval
- Reject/retry mechanism
- Iterate on prompts based on testing

### 11.3 S3 Costs
**Risk:** Large number of high-res images could increase storage costs
**Mitigation:**
- Compress images before upload (Pillow)
- Implement S3 lifecycle policies to archive old books
- Monitor costs via AWS billing alerts

### 11.4 LangGraph Execution Time
**Risk:** Processing 50+ page book may take too long
**Mitigation:**
- Run asynchronously (202 Accepted response)
- Show progress indicator in UI
- Consider pagination/chunking for very large books

---

## 12. Success Criteria

### MVP is considered successful when:

1. ✅ Admin can log in to `/admin`
2. ✅ Admin can create a new book with metadata
3. ✅ Admin can upload 20+ pages sequentially
4. ✅ OCR extracts text with >90% accuracy (manual verification)
5. ✅ Admin can approve pages and see them in sidebar
6. ✅ Admin can mark book complete and trigger guideline generation
7. ✅ LangGraph generates guideline.json with all required fields
8. ✅ Admin can review and approve guideline
9. ✅ teaching_guidelines table populated correctly (verify DB query)
10. ✅ Existing AI tutor can use new guidelines to teach (integration test)
11. ✅ Full NCERT Math Magic book processed end-to-end
12. ✅ All unit and integration tests passing
13. ✅ No major bugs in UI or backend
14. ✅ System can handle re-uploads, rejections, and retries

---

## 13. Implementation Progress & Status

**🎉 PHASES 1-5 COMPLETE! Backend & Frontend Fully Functional! 🎉**

### ✅ **Completed Phases (71% Complete)**

#### **Phase 1: Database Schema & Migrations** ✅ COMPLETE
- Created `books` table with full metadata and status tracking
- Created `book_guidelines` table for versioning
- Extended `teaching_guidelines` table with `book_id` and `source_pages` columns
- Implemented safe migration script with rollback support
- **Location:** `llm-backend/features/book_ingestion/migrations.py`
- **Test:** `python -m features.book_ingestion.migrations --migrate`

#### **Phase 2: AWS S3 Infrastructure** ✅ COMPLETE
- Created S3 bucket: `learnlikemagic-books` in `us-east-1`
- Implemented full S3 client with upload, download, presigned URLs, JSON handling
- Added dependencies: `boto3`, `Pillow`, `python-multipart`
- **Location:** `llm-backend/features/book_ingestion/utils/s3_client.py`
- **Test:** `python -m features.book_ingestion.utils.create_bucket`

#### **Phase 3: Book Management Backend APIs** ✅ COMPLETE
- Implemented BookService with full CRUD operations
- Created RESTful API endpoints:
  - `POST /admin/books` - Create book
  - `GET /admin/books` - List with filters
  - `GET /admin/books/{id}` - Get book details
  - `PUT /admin/books/{id}/status` - Update status
  - `DELETE /admin/books/{id}` - Delete book
- Implemented status state machine with validation
- **Location:** `llm-backend/features/book_ingestion/api/routes.py`
- **Test:** See "Testing Instructions" below

#### **Phase 4: OCR Service & Page Upload** ✅ COMPLETE
- Implemented OCRService using OpenAI Vision API (`gpt-4o-mini`)
- Created PageService with complete upload workflow
- Image validation (format, size), automatic PNG conversion
- Page upload, OCR extraction, approval/rejection workflow
- API endpoints:
  - `POST /admin/books/{id}/pages` - Upload page with auto-OCR
  - `PUT /admin/books/{id}/pages/{num}/approve` - Approve page
  - `DELETE /admin/books/{id}/pages/{num}` - Reject page
  - `GET /admin/books/{id}/pages/{num}` - Get page details
- **Location:** `llm-backend/features/book_ingestion/services/`
- **Test:** `python -m features.book_ingestion.tests.test_page_upload`

#### **Phase 5: Admin Frontend UI** ✅ COMPLETE
- Built complete React admin interface with TypeScript
- Implemented routing with React Router
- Created pages:
  - `/admin/books` - Books dashboard with filters
  - `/admin/books/new` - Create book form
  - `/admin/books/{id}` - Book detail with page management
- Features:
  - Drag-and-drop image upload
  - Side-by-side OCR review (image + extracted text)
  - Real-time status tracking
  - Responsive design
- **Location:** `llm-frontend/src/features/admin/`

### ⏳ **Remaining Phases (29% Remaining)**

#### **Phase 6: LangGraph Guideline Extraction** 🔄 NEXT
- Design LangGraph state machine for guideline extraction
- Implement incremental page-by-page processing
- Create nodes:
  - `extract_topics_node` - Identify main topics/chapters
  - `extract_subtopics_node` - Extract subtopics with context
  - `extract_learning_objectives_node` - Determine learning goals
  - `identify_misconceptions_node` - Find common errors
  - `extract_assessment_criteria_node` - Generate rubrics
  - `synthesize_guideline_node` - Combine into guideline.json
- Add API endpoints:
  - `POST /admin/books/{id}/generate-guidelines`
  - `GET /admin/books/{id}/guidelines`
  - `PUT /admin/books/{id}/guidelines/approve`
  - `PUT /admin/books/{id}/guidelines/reject`
- Implement auto-population logic to teaching_guidelines table
- **Estimated Time:** ~4 hours

#### **Phase 7: Testing & Integration** 🔄 FINAL
- End-to-end test with real NCERT Math Magic Grade 3 book
- Verify complete workflow: upload → OCR → guidelines → teaching_guidelines
- Test AI tutor integration with new guidelines
- Performance testing (OCR speed, guideline generation time)
- Bug fixes and refinements
- **Estimated Time:** ~2 hours

---

## 14. Testing Instructions

### **Prerequisites**
```bash
# Backend dependencies installed
cd llm-backend
source venv/bin/activate
pip install -r requirements.txt

# Frontend dependencies installed
cd llm-frontend
npm install

# Database migrated
cd llm-backend
python -m features.book_ingestion.migrations --migrate

# AWS S3 bucket created
python -m features.book_ingestion.utils.create_bucket
```

### **Start the Application**

**Terminal 1 - Backend:**
```bash
cd llm-backend
source venv/bin/activate
export PYTHONPATH=/Users/preethijain/manish/repos/learnlikemagic/llm-backend:$PYTHONPATH
uvicorn main:app --reload
# Server runs at http://localhost:8000
# API docs at http://localhost:8000/docs
```

**Terminal 2 - Frontend:**
```bash
cd llm-frontend
npm run dev
# Frontend runs at http://localhost:5173
```

### **Access the Application**

1. **Student Interface (Existing):** http://localhost:5173/
2. **Admin Dashboard:** http://localhost:5173/admin/books

### **Test Workflow**

#### **1. Create a Book**
- Navigate to http://localhost:5173/admin/books
- Click "Create New Book"
- Fill in details:
  - Title: "Math Magic"
  - Author: "NCERT"
  - Edition: "2024"
  - Country: "India"
  - Board: "CBSE"
  - Grade: 3
  - Subject: "Mathematics"
- Click "Create Book"

#### **2. Upload Pages**
- Click on the created book
- Drag and drop an image or click to upload
- Wait for OCR processing (~5-10 seconds)
- Review extracted text side-by-side with image
- Click "Approve Page" if correct, or "Reject & Re-upload" to try again
- Repeat for multiple pages

#### **3. Mark Book Complete**
- After uploading and approving pages
- Click "Mark Book Complete"
- Book status changes to "pages_complete"

#### **4. Generate Guidelines (Phase 6 - Coming Soon)**
- Currently shows "Coming Soon" placeholder
- Will trigger LangGraph workflow
- Admin reviews and approves generated guidelines
- System auto-populates teaching_guidelines table

### **API Testing with curl**

```bash
# Create a book
curl -X POST "http://localhost:8000/admin/books" \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Math Magic",
    "author": "NCERT",
    "country": "India",
    "board": "CBSE",
    "grade": 3,
    "subject": "Mathematics"
  }'

# List books
curl "http://localhost:8000/admin/books?country=India&board=CBSE&grade=3"

# Get book details
curl "http://localhost:8000/admin/books/ncert_mathematics_3_2024"

# Upload a page
curl -X POST "http://localhost:8000/admin/books/ncert_mathematics_3_2024/pages" \
  -F "image=@/path/to/page1.png"

# Approve a page
curl -X PUT "http://localhost:8000/admin/books/ncert_mathematics_3_2024/pages/1/approve"
```

### **Automated Testing**

```bash
# Run page upload test
cd llm-backend
source venv/bin/activate
python -m features.book_ingestion.tests.test_page_upload
```

---

## 15. Project Structure

### **Backend**
```
llm-backend/
├── features/                      # ← NEW: Feature modules
│   └── book_ingestion/            # ← Book ingestion feature (isolated)
│       ├── __init__.py
│       ├── api/
│       │   └── routes.py          # Admin API endpoints
│       ├── models/
│       │   ├── database.py        # SQLAlchemy ORM models
│       │   └── schemas.py         # Pydantic request/response schemas
│       ├── repositories/
│       │   ├── book_repository.py
│       │   └── book_guideline_repository.py
│       ├── services/
│       │   ├── book_service.py    # Book business logic
│       │   ├── ocr_service.py     # OpenAI Vision API integration
│       │   └── page_service.py    # Page upload workflow
│       ├── utils/
│       │   ├── s3_client.py       # AWS S3 operations
│       │   ├── setup_s3.py
│       │   └── create_bucket.py
│       ├── graph/                 # Ready for Phase 6
│       │   └── guideline_extraction/
│       │       └── prompts/
│       ├── tests/
│       │   ├── test_page_upload.py
│       │   ├── unit/
│       │   └── integration/
│       └── migrations.py          # Database migrations
│
├── main.py                        # ← MODIFIED: Added admin router
├── config.py                      # ← MODIFIED: Added AWS settings
├── requirements.txt               # ← MODIFIED: Added boto3, Pillow, etc.
└── [existing files unchanged]
```

### **Frontend**
```
llm-frontend/src/
├── features/                      # ← NEW: Feature modules
│   └── admin/                     # ← Admin feature
│       ├── api/
│       │   └── adminApi.ts        # API client for admin endpoints
│       ├── types/
│       │   └── index.ts           # TypeScript type definitions
│       ├── components/
│       │   ├── BookStatusBadge.tsx
│       │   ├── PageUploadPanel.tsx
│       │   └── PagesSidebar.tsx
│       ├── pages/
│       │   ├── BooksDashboard.tsx
│       │   ├── CreateBook.tsx
│       │   └── BookDetail.tsx
│       └── hooks/
│           └── useAdminAuth.ts    # (Reserved for future auth)
│
├── App.tsx                        # ← NEW: Routing setup
├── TutorApp.tsx                   # ← RENAMED: Was App.tsx
└── [existing files unchanged]
```

---

## 16. Key Design Decisions

### **✅ Modular Architecture**
- Complete feature isolation under `features/book_ingestion/`
- Zero breaking changes to existing code
- Easy to remove or extend independently

### **✅ Database Design**
- Separate tables for books and guidelines
- Extended teaching_guidelines without breaking existing queries
- Status state machine enforced at DB and API level

### **✅ S3 Storage Structure**
```
s3://learnlikemagic-books/
└── books/
    └── {book_id}/
        ├── metadata.json          # Page list and status
        ├── 1.png                  # Page images
        ├── 1.txt                  # OCR text
        ├── 2.png
        ├── 2.txt
        └── guideline.json         # Generated guidelines
```

### **✅ Security & Error Handling**
- Input validation at API layer (Pydantic)
- Error handling with proper HTTP status codes
- AWS credentials auto-detection (no hardcoding)
- Image validation (format, size)

### **✅ User Experience**
- Side-by-side OCR review
- Drag-and-drop upload
- Real-time status tracking
- Clear error messages

---

## 17. Next Steps

### **Immediate (Phase 6 - ~4 hours)**
1. Design LangGraph state machine
2. Implement extraction nodes with prompts
3. Add guideline generation endpoints
4. Build guideline review UI
5. Implement auto-population logic

### **Final (Phase 7 - ~2 hours)**
1. End-to-end test with real NCERT book
2. Verify AI tutor integration
3. Performance optimization
4. Bug fixes
5. Documentation updates

### **Post-MVP Enhancements**
- Batch page upload
- Inline OCR text editing
- Multi-book guideline merging
- Topic synonym resolution
- Analytics dashboard

---

*Document last updated: 2025-10-26*
*Status: 🚀 71% COMPLETE - Phases 1-5 done, backend & frontend fully functional!*
*Current Phase: Ready to start Phase 6 (LangGraph Guideline Extraction)*
