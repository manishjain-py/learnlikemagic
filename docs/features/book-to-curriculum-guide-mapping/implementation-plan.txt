# Learn Like Magic - Book Ingestion & Guideline Generation
# Implementation Plan

## Document Information
- **Created:** 2025-10-25
- **Status:** Draft - Awaiting AWS Credentials & OpenAI Vision Reference Code
- **Target:** MVP for CBSE Grade 3 Mathematics (NCERT Math Magic)

---

## 1. Executive Summary

This implementation plan details the technical approach to build the Book Ingestion & Guideline Generation system for Learn Like Magic. The system will enable admins to:

1. Upload textbook pages as images
2. Extract text via OpenAI Vision OCR
3. Review and approve OCR output
4. Generate teaching guidelines using LangGraph
5. Automatically populate the existing `teaching_guidelines` table

**Key Architecture Decisions:**
- S3 for file storage (`learnlikemagic-books` bucket)
- PostgreSQL for metadata and state tracking
- Admin UI embedded in existing React frontend
- LangGraph for guideline extraction workflow
- Subtopic-level granularity maintained in teaching_guidelines table

---

## 2. Implementation Phases

### Phase 1: Database Schema & Migrations (Can start immediately)
**Duration:** 1-2 days
**Blockers:** None

**Tasks:**
1. Create database migration scripts (Alembic)
2. Add new tables: `books`, `book_guidelines`
3. Modify existing table: `teaching_guidelines` (add `book_id`, `source_pages` columns)
4. Update ORM models in `models/database.py`
5. Update Pydantic schemas in `models/schemas.py`
6. Create repository classes for new tables

**Deliverables:**
- Migration files in `llm-backend/alembic/versions/`
- Updated `models/database.py`
- New `models/schemas.py` with Book* models
- `repositories/book_repository.py`
- `repositories/book_guideline_repository.py`

---

### Phase 2: AWS S3 Infrastructure
**Duration:** 1 day
**Blockers:** None (AWS credentials in ~/.aws/credentials)

**Tasks:**
1. Add `boto3` and `Pillow` to `requirements.txt`
2. Add AWS config to `config.py` (region, bucket name - credentials auto-detected)
3. Create `utils/s3_client.py` with S3 operations:
   - `upload_file(local_path, s3_key)` → S3 URL
   - `download_file(s3_key, local_path)` → file
   - `get_file_url(s3_key)` → presigned URL
   - `delete_file(s3_key)` → bool
   - `update_metadata_json(book_id, metadata)` → bool
4. Create S3 bucket `learnlikemagic-books` (if doesn't exist)
5. Set up folder structure: `books/{book_id}/`

**Deliverables:**
- `utils/s3_client.py` with full CRUD operations
- Updated `config.py` with AWS settings
- S3 bucket created and verified

---

### Phase 3: Book Management Backend APIs
**Duration:** 2-3 days
**Dependencies:** Phase 1 complete
**Blockers:** None (can use local filesystem for testing if S3 not ready)

**Tasks:**
1. Create `services/book_service.py` with business logic
2. Create `api/routes/admin.py` with book CRUD endpoints
3. Implement book status state machine transitions
4. Add book metadata extraction placeholder (will integrate Vision API in Phase 4)

**API Endpoints:**

```
POST   /admin/books                    - Create new book
GET    /admin/books                    - List all books (with filters)
GET    /admin/books/{book_id}          - Get book details + pages
PUT    /admin/books/{book_id}          - Update book metadata
DELETE /admin/books/{book_id}          - Delete book + S3 files
PUT    /admin/books/{book_id}/status   - Update book status
```

**Request/Response Models:**

```python
# Create Book Request
{
  "title": "Math Magic",
  "author": "NCERT",
  "edition": "2024",
  "edition_year": 2024,
  "country": "India",
  "board": "CBSE",
  "grade": 3,
  "subject": "Mathematics"
}

# Book Response
{
  "id": "ncert_math_3_2024",
  "title": "Math Magic",
  "author": "NCERT",
  "status": "draft",
  "pages": [],
  "s3_prefix": "books/ncert_math_3_2024/",
  "created_at": "2025-10-25T10:00:00Z",
  "metadata": { ... }
}
```

**Deliverables:**
- `services/book_service.py`
- `api/routes/admin.py`
- Unit tests in `tests/unit/test_book_service.py`
- Integration tests in `tests/integration/test_book_api.py`

---

### Phase 4: OCR & Page Upload (Blocked: OpenAI Vision API reference code)
**Duration:** 2-3 days
**Dependencies:** Phase 2, Phase 3
**Blockers:** OpenAI Vision API reference code

**Tasks:**
1. Create `services/ocr_service.py` with Vision API integration
2. Add page upload endpoints to `api/routes/admin.py`
3. Implement image validation (format, size)
4. Store images and OCR text in S3
5. Update `metadata.json` in S3 after each approval

**API Endpoints:**

```
POST   /admin/books/{book_id}/pages                    - Upload page image
POST   /admin/books/{book_id}/pages/{page_num}/ocr     - Trigger OCR (may be auto)
PUT    /admin/books/{book_id}/pages/{page_num}/approve - Approve page
DELETE /admin/books/{book_id}/pages/{page_num}         - Reject/delete page (allows re-upload)
GET    /admin/books/{book_id}/pages                    - List all pages with status
GET    /admin/books/{book_id}/pages/{page_num}         - Get page details + presigned URLs
```

**Page Upload Flow:**

```python
1. POST /admin/books/{book_id}/pages
   - Request: multipart/form-data with image file
   - Validate: format (PNG/JPG/TIFF), size (<10MB)
   - Generate page_num (next available)
   - Store: s3://learnlikemagic-books/books/{book_id}/{page_num}.png
   - Auto-trigger OCR via Vision API
   - Store OCR text: books/{book_id}/{page_num}.txt
   - Return: page_num, image_url, ocr_text, status: "pending_review"

2. PUT /admin/books/{book_id}/pages/{page_num}/approve
   - Update metadata.json in S3 with approved status
   - Update book status to "uploading_pages" if first page
   - Return: success

3. DELETE /admin/books/{book_id}/pages/{page_num}
   - Delete image and text from S3
   - Remove from metadata.json
   - Allow re-upload at same page_num
```

**metadata.json Structure:**

```json
{
  "book_id": "ncert_math_3_2024",
  "pages": [
    {
      "page_num": 1,
      "image_s3_key": "books/ncert_math_3_2024/1.png",
      "text_s3_key": "books/ncert_math_3_2024/1.txt",
      "status": "approved",
      "approved_at": "2025-10-25T10:30:00Z"
    },
    {
      "page_num": 2,
      "image_s3_key": "books/ncert_math_3_2024/2.png",
      "text_s3_key": "books/ncert_math_3_2024/2.txt",
      "status": "approved",
      "approved_at": "2025-10-25T10:35:00Z"
    }
  ],
  "total_pages": 2,
  "last_updated": "2025-10-25T10:35:00Z"
}
```

**OCR Service:**

```python
# services/ocr_service.py
class OCRService:
    def __init__(self):
        self.client = OpenAI(api_key=settings.openai_api_key)

    def extract_text_from_image(self, image_path: str) -> str:
        """
        Use OpenAI Vision API to extract text from textbook page.

        Reference code to be provided by user.
        """
        # Implementation based on reference code
        pass
```

**Deliverables:**
- `services/ocr_service.py`
- Updated `api/routes/admin.py` with page endpoints
- Image upload handling with validation
- S3 metadata.json management
- Tests for OCR service

---

### Phase 5: Admin Frontend UI
**Duration:** 3-4 days
**Dependencies:** Phase 3, Phase 4
**Blockers:** None (can mock API responses)

**Tasks:**
1. Add admin routes to `llm-frontend/src/App.tsx`
2. Create admin page components
3. Implement hardcoded auth (admin/admin)
4. Build book management dashboard
5. Build page upload interface with side-by-side review
6. Implement approved pages sidebar
7. Add file upload with preview

**Component Structure:**

```
llm-frontend/src/
├── pages/
│   └── admin/
│       ├── AdminLogin.tsx           - Login form (admin/admin)
│       ├── BooksDashboard.tsx       - List books, create new
│       ├── CreateBook.tsx           - New book form
│       ├── BookDetail.tsx           - Main book management page
│       │   ├── PageUploadPanel.tsx  - Left: upload + review
│       │   ├── PagesSidebar.tsx     - Right: approved pages list
│       │   └── GuidelinesPanel.tsx  - Bottom: guideline review (Phase 6)
│       └── components/
│           ├── BookCard.tsx         - Book list item
│           ├── BookStatusBadge.tsx  - Status indicator
│           ├── ImageTextViewer.tsx  - Side-by-side image/text
│           └── FileUploader.tsx     - Drag-n-drop uploader
├── api/
│   └── adminApi.ts                  - Admin API client
└── hooks/
    └── useAdminAuth.ts              - Simple auth hook
```

**Key UI Screens:**

1. **Login Page (`/admin`)**
   ```tsx
   - Username input (default: admin)
   - Password input (default: admin)
   - Login button
   - On success: store token in localStorage, redirect to /admin/books
   ```

2. **Books Dashboard (`/admin/books`)**
   ```tsx
   - Header: "Books" + "Create New Book" button
   - Filter bar: Board, Grade, Subject, Status
   - Book cards grid:
     - Cover image thumbnail
     - Title, Author, Grade, Subject
     - Status badge
     - Page count
     - Click to open detail
   ```

3. **Create Book (`/admin/books/new`)**
   ```tsx
   - Form fields: Title, Author, Edition, Year, Country, Board, Grade, Subject
   - Cover image upload (optional for MVP)
   - Submit → POST /admin/books → redirect to book detail
   ```

4. **Book Detail (`/admin/books/{id}`)**
   ```tsx
   - Header:
     - Book metadata (title, author, etc.)
     - Status badge (with transitions)
     - Actions: "Mark Complete", "Generate Guidelines"

   - Layout: 3-column

     [Left Panel: Page Upload (40%)]
     - If status = "draft" or "uploading_pages":
       - File upload dropzone
       - "Upload Page" button
       - After upload:
         - Image preview (top)
         - OCR text (bottom)
         - Approve / Reject buttons

     [Middle Panel: Page Viewer (30%)]
     - Selected page display (from sidebar)
     - Image + text in tabs or split view

     [Right Sidebar: Approved Pages (30%)]
     - Scrollable list of approved pages
     - Each item: thumbnail + page number
     - Click to view in middle panel

   - Bottom Panel: Guidelines (appears when status = "guidelines_pending_review")
     - JSON viewer with syntax highlighting
     - Approve / Reject buttons
   ```

**State Management:**

```tsx
// BookDetail.tsx
const [book, setBook] = useState<Book | null>(null);
const [currentPage, setCurrentPage] = useState<number | null>(null);
const [uploadedImage, setUploadedImage] = useState<File | null>(null);
const [ocrText, setOcrText] = useState<string>("");
const [isProcessing, setIsProcessing] = useState(false);

// Upload flow
const handleImageUpload = async (file: File) => {
  setUploadedImage(file);
  setIsProcessing(true);

  const formData = new FormData();
  formData.append('image', file);

  const result = await adminApi.uploadPage(bookId, formData);
  setCurrentPage(result.page_num);
  setOcrText(result.ocr_text);
  setIsProcessing(false);
};

const handleApprovePage = async () => {
  await adminApi.approvePage(bookId, currentPage);
  // Refresh book data
  fetchBook();
  // Reset upload form
  setUploadedImage(null);
  setOcrText("");
};
```

**Deliverables:**
- Complete admin UI with all screens
- API integration
- File upload with preview
- Side-by-side review interface
- Responsive design (desktop-first for MVP)

---

### Phase 6: Guideline Generation (LangGraph Workflow)
**Duration:** 3-4 days
**Dependencies:** Phase 4 (need approved pages)
**Blockers:** None

**Tasks:**
1. Design LangGraph state schema for guideline extraction
2. Create graph nodes for each extraction step
3. Create prompts for each node
4. Implement incremental page processing with context
5. Add guideline generation endpoint
6. Implement guideline approval endpoint with auto-population logic

**LangGraph Architecture:**

```
graph/
├── guideline_extraction/
│   ├── state.py                 - GuidelineState definition
│   ├── nodes.py                 - All node implementations
│   ├── build_graph.py           - Graph compilation
│   └── utils.py                 - Helper functions
└── prompts/
    └── guideline_extraction/
        ├── extract_topics.txt
        ├── extract_subtopics.txt
        ├── extract_objectives.txt
        ├── identify_misconceptions.txt
        └── synthesize_guideline.txt
```

**State Schema:**

```python
# graph/guideline_extraction/state.py
from typing import TypedDict, List, Dict, Any

class GuidelineState(TypedDict):
    book_id: str
    book_metadata: Dict[str, Any]
    pages: List[Dict[str, Any]]  # [{"page_num": 1, "text": "..."}]
    current_page_idx: int

    # Accumulated extractions
    topics: List[Dict[str, Any]]  # [{"topic": "Fractions", "subtopics": [...]}]
    current_topic: str
    context_from_previous_page: str

    # Final output
    guideline_json: Dict[str, Any]
    error: str | None
```

**Graph Nodes:**

```python
# graph/guideline_extraction/nodes.py

def extract_topics_node(state: GuidelineState) -> GuidelineState:
    """
    Analyze all page texts to identify main topics/chapters.

    Input: All page texts
    Output: List of topic names
    """
    all_text = "\n\n".join([p["text"] for p in state["pages"]])

    prompt = load_prompt("extract_topics")
    response = llm.invoke(prompt.format(
        book_metadata=state["book_metadata"],
        all_text=all_text
    ))

    topics = parse_topics_from_response(response)
    state["topics"] = topics
    return state


def extract_subtopics_node(state: GuidelineState) -> GuidelineState:
    """
    For each topic, extract subtopics incrementally page-by-page.

    Uses context from previous pages to detect:
    - Topic continuation
    - New subtopic start
    - Topic end
    """
    # Process pages sequentially
    for page_idx, page in enumerate(state["pages"]):
        context = state.get("context_from_previous_page", "")

        prompt = load_prompt("extract_subtopics")
        response = llm.invoke(prompt.format(
            current_page_text=page["text"],
            previous_context=context,
            identified_topics=state["topics"],
            page_number=page["page_num"]
        ))

        # Update topics with new subtopics
        updated_topics = parse_subtopics_from_response(response)
        state["topics"] = merge_topics(state["topics"], updated_topics)

        # Store context for next page
        state["context_from_previous_page"] = generate_context_summary(
            page["text"],
            updated_topics
        )

    return state


def extract_learning_objectives_node(state: GuidelineState) -> GuidelineState:
    """
    For each subtopic, determine learning objectives.
    """
    for topic in state["topics"]:
        for subtopic in topic["subtopics"]:
            # Get relevant page texts for this subtopic
            relevant_text = get_text_for_pages(
                state["pages"],
                subtopic["source_pages"]
            )

            prompt = load_prompt("extract_objectives")
            response = llm.invoke(prompt.format(
                topic=topic["topic"],
                subtopic=subtopic["subtopic"],
                content=relevant_text,
                grade=state["book_metadata"]["grade"]
            ))

            objectives = parse_objectives_from_response(response)
            subtopic["metadata"]["learning_objectives"] = objectives

    return state


def identify_misconceptions_node(state: GuidelineState) -> GuidelineState:
    """
    Identify common misconceptions for each subtopic.
    """
    for topic in state["topics"]:
        for subtopic in topic["subtopics"]:
            relevant_text = get_text_for_pages(
                state["pages"],
                subtopic["source_pages"]
            )

            prompt = load_prompt("identify_misconceptions")
            response = llm.invoke(prompt.format(
                topic=topic["topic"],
                subtopic=subtopic["subtopic"],
                content=relevant_text
            ))

            misconceptions = parse_misconceptions_from_response(response)
            subtopic["metadata"]["common_misconceptions"] = misconceptions

    return state


def extract_assessment_criteria_node(state: GuidelineState) -> GuidelineState:
    """
    Generate assessment criteria and example problems.
    """
    # Similar pattern to above nodes
    pass


def synthesize_guideline_node(state: GuidelineState) -> GuidelineState:
    """
    Combine all extracted data into final guideline.json structure.
    """
    guideline_json = {
        "book_id": state["book_id"],
        "book_metadata": state["book_metadata"],
        "topics": state["topics"]
    }

    state["guideline_json"] = guideline_json
    return state
```

**Graph Compilation:**

```python
# graph/guideline_extraction/build_graph.py
from langgraph.graph import StateGraph, END

def build_guideline_extraction_graph():
    workflow = StateGraph(GuidelineState)

    # Add nodes
    workflow.add_node("extract_topics", extract_topics_node)
    workflow.add_node("extract_subtopics", extract_subtopics_node)
    workflow.add_node("extract_objectives", extract_learning_objectives_node)
    workflow.add_node("identify_misconceptions", identify_misconceptions_node)
    workflow.add_node("extract_assessment", extract_assessment_criteria_node)
    workflow.add_node("synthesize", synthesize_guideline_node)

    # Define flow
    workflow.set_entry_point("extract_topics")
    workflow.add_edge("extract_topics", "extract_subtopics")
    workflow.add_edge("extract_subtopics", "extract_objectives")
    workflow.add_edge("extract_objectives", "identify_misconceptions")
    workflow.add_edge("identify_misconceptions", "extract_assessment")
    workflow.add_edge("extract_assessment", "synthesize")
    workflow.add_edge("synthesize", END)

    return workflow.compile()
```

**API Endpoints:**

```
POST   /admin/books/{book_id}/generate-guidelines   - Trigger guideline generation
GET    /admin/books/{book_id}/guidelines            - Get generated guideline.json
PUT    /admin/books/{book_id}/guidelines/approve    - Approve & populate teaching_guidelines
PUT    /admin/books/{book_id}/guidelines/reject     - Reject & allow retry
```

**Guideline Approval Logic:**

```python
# services/book_service.py
def approve_guideline(book_id: str) -> bool:
    """
    Approve guideline and populate teaching_guidelines table.
    """
    # 1. Load guideline.json from S3
    guideline_json = s3_client.get_json(f"books/{book_id}/guideline.json")

    # 2. Parse and create teaching_guideline rows
    book = book_repo.get_by_id(book_id)

    for topic in guideline_json["topics"]:
        for subtopic_data in topic["subtopics"]:
            teaching_guideline = TeachingGuideline(
                id=generate_id(),
                country=book.country,
                board=book.board,
                grade=book.grade,
                subject=book.subject,
                topic=topic["topic"],
                subtopic=subtopic_data["subtopic"],
                guideline=subtopic_data["guideline"],
                metadata_json=json.dumps(subtopic_data["metadata"]),
                book_id=book_id,
                source_pages=json.dumps(subtopic_data["source_pages"])
            )
            guideline_repo.create(teaching_guideline)

    # 3. Update book status
    book_repo.update_status(book_id, "approved")

    # 4. Update book_guidelines status
    book_guideline_repo.update_status(book_id, "approved")

    return True
```

**Deliverables:**
- Complete LangGraph workflow
- All node implementations with prompts
- Guideline generation API endpoint
- Approval logic with auto-population
- Tests for graph execution
- Integration test with sample book

---

### Phase 7: Testing & Integration
**Duration:** 2-3 days
**Dependencies:** All previous phases

**Tasks:**
1. End-to-end testing with NCERT Math Magic Grade 3
2. Unit tests for all services and repositories
3. Integration tests for API endpoints
4. Manual UI testing
5. Performance testing (OCR speed, S3 upload time)
6. Bug fixes and refinements

**Test Coverage:**

```
Unit Tests:
- book_service.py: CRUD operations, status transitions
- ocr_service.py: Mock Vision API calls
- s3_client.py: Mock boto3 operations
- book_repository.py: Database operations

Integration Tests:
- POST /admin/books → verify DB insert + S3 folder creation
- POST /admin/books/{id}/pages → verify OCR + S3 storage
- PUT /admin/books/{id}/pages/{num}/approve → verify metadata update
- POST /admin/books/{id}/generate-guidelines → verify LangGraph execution
- PUT /admin/books/{id}/guidelines/approve → verify teaching_guidelines population

E2E Test:
- Full workflow: Create book → Upload 5 pages → Approve all → Generate guidelines → Approve → Verify AI tutor can query
```

**Deliverables:**
- 80%+ test coverage
- E2E test passing with sample book
- Performance benchmarks documented
- Bug fixes applied
- User acceptance testing completed

---

## 3. Database Schema (SQL)

```sql
-- New table: books
CREATE TABLE books (
    id VARCHAR PRIMARY KEY,
    title VARCHAR NOT NULL,
    author VARCHAR,
    edition VARCHAR,
    edition_year INTEGER,
    country VARCHAR NOT NULL,
    board VARCHAR NOT NULL,
    grade INTEGER NOT NULL,
    subject VARCHAR NOT NULL,
    cover_image_s3_key VARCHAR,
    s3_prefix VARCHAR NOT NULL,
    status VARCHAR NOT NULL CHECK (status IN (
        'draft',
        'uploading_pages',
        'pages_complete',
        'generating_guidelines',
        'guidelines_pending_review',
        'approved'
    )),
    metadata_s3_key VARCHAR,  -- books/{book_id}/metadata.json
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    created_by VARCHAR DEFAULT 'admin'
);

CREATE INDEX idx_books_curriculum ON books(country, board, grade, subject);
CREATE INDEX idx_books_status ON books(status);


-- New table: book_guidelines
CREATE TABLE book_guidelines (
    id VARCHAR PRIMARY KEY,
    book_id VARCHAR NOT NULL REFERENCES books(id) ON DELETE CASCADE,
    guideline_s3_key VARCHAR NOT NULL,  -- books/{book_id}/guideline.json
    status VARCHAR NOT NULL CHECK (status IN (
        'draft',
        'pending_review',
        'approved',
        'rejected'
    )),
    generated_at TIMESTAMP,
    reviewed_at TIMESTAMP,
    reviewed_by VARCHAR,
    version INTEGER DEFAULT 1,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_book_guidelines_book ON book_guidelines(book_id);


-- Modify existing table: teaching_guidelines
ALTER TABLE teaching_guidelines
ADD COLUMN book_id VARCHAR REFERENCES books(id) ON DELETE SET NULL,
ADD COLUMN source_pages VARCHAR;  -- JSON array: "[15, 16, 17]"

CREATE INDEX idx_teaching_guidelines_book ON teaching_guidelines(book_id);
```

**Alembic Migration:**

```python
# alembic/versions/001_add_book_ingestion_tables.py
def upgrade():
    # Create books table
    op.create_table(
        'books',
        sa.Column('id', sa.String(), nullable=False),
        sa.Column('title', sa.String(), nullable=False),
        sa.Column('author', sa.String(), nullable=True),
        sa.Column('edition', sa.String(), nullable=True),
        sa.Column('edition_year', sa.Integer(), nullable=True),
        sa.Column('country', sa.String(), nullable=False),
        sa.Column('board', sa.String(), nullable=False),
        sa.Column('grade', sa.Integer(), nullable=False),
        sa.Column('subject', sa.String(), nullable=False),
        sa.Column('cover_image_s3_key', sa.String(), nullable=True),
        sa.Column('s3_prefix', sa.String(), nullable=False),
        sa.Column('status', sa.String(), nullable=False),
        sa.Column('metadata_s3_key', sa.String(), nullable=True),
        sa.Column('created_at', sa.DateTime(), server_default=sa.func.now()),
        sa.Column('updated_at', sa.DateTime(), server_default=sa.func.now()),
        sa.Column('created_by', sa.String(), server_default='admin'),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_books_curriculum', 'books', ['country', 'board', 'grade', 'subject'])
    op.create_index('idx_books_status', 'books', ['status'])

    # Create book_guidelines table
    op.create_table(
        'book_guidelines',
        sa.Column('id', sa.String(), nullable=False),
        sa.Column('book_id', sa.String(), nullable=False),
        sa.Column('guideline_s3_key', sa.String(), nullable=False),
        sa.Column('status', sa.String(), nullable=False),
        sa.Column('generated_at', sa.DateTime(), nullable=True),
        sa.Column('reviewed_at', sa.DateTime(), nullable=True),
        sa.Column('reviewed_by', sa.String(), nullable=True),
        sa.Column('version', sa.Integer(), server_default='1'),
        sa.Column('created_at', sa.DateTime(), server_default=sa.func.now()),
        sa.PrimaryKeyConstraint('id'),
        sa.ForeignKeyConstraint(['book_id'], ['books.id'], ondelete='CASCADE')
    )
    op.create_index('idx_book_guidelines_book', 'book_guidelines', ['book_id'])

    # Modify teaching_guidelines table
    op.add_column('teaching_guidelines',
                  sa.Column('book_id', sa.String(), nullable=True))
    op.add_column('teaching_guidelines',
                  sa.Column('source_pages', sa.String(), nullable=True))
    op.create_foreign_key(
        'fk_teaching_guidelines_book',
        'teaching_guidelines',
        'books',
        ['book_id'],
        ['id'],
        ondelete='SET NULL'
    )
    op.create_index('idx_teaching_guidelines_book',
                    'teaching_guidelines',
                    ['book_id'])

def downgrade():
    # Reverse all changes
    op.drop_index('idx_teaching_guidelines_book', 'teaching_guidelines')
    op.drop_constraint('fk_teaching_guidelines_book', 'teaching_guidelines')
    op.drop_column('teaching_guidelines', 'source_pages')
    op.drop_column('teaching_guidelines', 'book_id')

    op.drop_index('idx_book_guidelines_book', 'book_guidelines')
    op.drop_table('book_guidelines')

    op.drop_index('idx_books_status', 'books')
    op.drop_index('idx_books_curriculum', 'books')
    op.drop_table('books')
```

---

## 4. API Specification

### 4.1 Admin Authentication
```
POST /admin/login
Request:
{
  "username": "admin",
  "password": "admin"
}

Response:
{
  "token": "simple-jwt-token",
  "username": "admin"
}
```

### 4.2 Book Management
```
POST /admin/books
Request:
{
  "title": "Math Magic",
  "author": "NCERT",
  "edition": "2024",
  "edition_year": 2024,
  "country": "India",
  "board": "CBSE",
  "grade": 3,
  "subject": "Mathematics"
}

Response: 201 Created
{
  "id": "ncert_math_3_2024",
  "title": "Math Magic",
  "status": "draft",
  "s3_prefix": "books/ncert_math_3_2024/",
  "created_at": "2025-10-25T10:00:00Z"
}

---

GET /admin/books?country=India&board=CBSE&grade=3

Response: 200 OK
{
  "books": [
    {
      "id": "ncert_math_3_2024",
      "title": "Math Magic",
      "author": "NCERT",
      "status": "uploading_pages",
      "page_count": 5,
      "created_at": "2025-10-25T10:00:00Z"
    }
  ],
  "total": 1
}

---

GET /admin/books/{book_id}

Response: 200 OK
{
  "id": "ncert_math_3_2024",
  "title": "Math Magic",
  "author": "NCERT",
  "status": "uploading_pages",
  "pages": [
    {
      "page_num": 1,
      "status": "approved",
      "image_url": "https://s3.../1.png",
      "text_url": "https://s3.../1.txt"
    }
  ],
  "metadata": { ... }
}

---

PUT /admin/books/{book_id}/status
Request:
{
  "status": "pages_complete"
}

Response: 200 OK
{
  "id": "ncert_math_3_2024",
  "status": "pages_complete"
}
```

### 4.3 Page Management
```
POST /admin/books/{book_id}/pages
Request: multipart/form-data
- image: File (PNG/JPG/TIFF)

Response: 201 Created
{
  "page_num": 1,
  "image_url": "https://s3-presigned-url.../1.png",
  "ocr_text": "Chapter 1: Fractions\n\nA fraction represents...",
  "status": "pending_review"
}

---

PUT /admin/books/{book_id}/pages/{page_num}/approve

Response: 200 OK
{
  "page_num": 1,
  "status": "approved"
}

---

DELETE /admin/books/{book_id}/pages/{page_num}

Response: 204 No Content
```

### 4.4 Guideline Generation
```
POST /admin/books/{book_id}/generate-guidelines

Response: 202 Accepted
{
  "message": "Guideline generation started",
  "estimated_time": "2-3 minutes"
}

---

GET /admin/books/{book_id}/guidelines

Response: 200 OK
{
  "book_id": "ncert_math_3_2024",
  "status": "pending_review",
  "guideline": {
    "book_id": "ncert_math_3_2024",
    "book_metadata": { ... },
    "topics": [ ... ]
  },
  "generated_at": "2025-10-25T11:00:00Z"
}

---

PUT /admin/books/{book_id}/guidelines/approve

Response: 200 OK
{
  "message": "Guideline approved and populated to teaching_guidelines",
  "teaching_guidelines_created": 6
}

---

PUT /admin/books/{book_id}/guidelines/reject
Request:
{
  "reason": "Missing subtopics for chapter 3"
}

Response: 200 OK
{
  "message": "Guideline rejected, can be regenerated"
}
```

---

## 5. LangGraph Workflow Details

### 5.1 Graph Visualization

```
                 ┌─────────────────────┐
                 │  START              │
                 └──────────┬──────────┘
                            │
                            ▼
                 ┌─────────────────────┐
                 │  extract_topics     │
                 │  (analyze all pages)│
                 └──────────┬──────────┘
                            │
                            ▼
                 ┌─────────────────────┐
                 │ extract_subtopics   │
                 │ (incremental, page  │
                 │  by page with ctx)  │
                 └──────────┬──────────┘
                            │
                            ▼
                 ┌─────────────────────┐
                 │ extract_objectives  │
                 │ (for each subtopic) │
                 └──────────┬──────────┘
                            │
                            ▼
                 ┌─────────────────────┐
                 │identify_misconceptions│
                 │ (per subtopic)      │
                 └──────────┬──────────┘
                            │
                            ▼
                 ┌─────────────────────┐
                 │extract_assessment   │
                 │ (criteria + examples)│
                 └──────────┬──────────┘
                            │
                            ▼
                 ┌─────────────────────┐
                 │  synthesize         │
                 │  (combine into JSON)│
                 └──────────┬──────────┘
                            │
                            ▼
                 ┌─────────────────────┐
                 │   END               │
                 └─────────────────────┘
```

### 5.2 Prompt Examples

**extract_subtopics.txt:**
```
You are an expert curriculum analyzer for grade {grade} {subject}.

TASK: Identify subtopics from this textbook page.

CONTEXT FROM PREVIOUS PAGE:
{previous_context}

CURRENT PAGE (Page {page_number}):
{current_page_text}

IDENTIFIED TOPICS SO FAR:
{identified_topics}

Analyze this page and determine:
1. Is this a continuation of a previous subtopic, or a new subtopic?
2. What is the subtopic name?
3. Which topic does this subtopic belong to?
4. What page numbers does this subtopic span?

Return your analysis in this JSON format:
{{
  "is_continuation": true/false,
  "subtopic": "Subtopic name",
  "topic": "Parent topic",
  "page_range_start": {page_number},
  "page_range_end": {page_number},  // Will be updated as we see more pages
  "key_concepts": ["concept1", "concept2"]
}}

IMPORTANT: Use the context from the previous page to make accurate decisions about topic boundaries.
```

---

## 6. Frontend Component Details

### 6.1 BookDetail.tsx (Main Component)

```tsx
// llm-frontend/src/pages/admin/BookDetail.tsx
import { useState, useEffect } from 'react';
import { useParams } from 'react-router-dom';
import { adminApi } from '../../api/adminApi';
import PageUploadPanel from './components/PageUploadPanel';
import PagesSidebar from './components/PagesSidebar';
import GuidelinesPanel from './components/GuidelinesPanel';

export default function BookDetail() {
  const { bookId } = useParams();
  const [book, setBook] = useState(null);
  const [selectedPage, setSelectedPage] = useState(null);

  useEffect(() => {
    loadBook();
  }, [bookId]);

  const loadBook = async () => {
    const data = await adminApi.getBook(bookId);
    setBook(data);
  };

  const handlePageApproved = () => {
    loadBook(); // Refresh to show new page in sidebar
  };

  const handleMarkComplete = async () => {
    await adminApi.updateBookStatus(bookId, 'pages_complete');
    loadBook();
  };

  const handleGenerateGuidelines = async () => {
    await adminApi.generateGuidelines(bookId);
    loadBook();
  };

  return (
    <div className="book-detail">
      {/* Header */}
      <div className="book-header">
        <h1>{book?.title}</h1>
        <StatusBadge status={book?.status} />
        <div className="actions">
          {book?.status === 'uploading_pages' && (
            <button onClick={handleMarkComplete}>
              Mark Book Complete
            </button>
          )}
          {book?.status === 'pages_complete' && (
            <button onClick={handleGenerateGuidelines}>
              Generate Guidelines
            </button>
          )}
        </div>
      </div>

      {/* Main Content */}
      <div className="book-content">
        {/* Left: Page Upload */}
        <div className="left-panel">
          <PageUploadPanel
            bookId={bookId}
            onPageApproved={handlePageApproved}
          />
        </div>

        {/* Right: Approved Pages */}
        <div className="right-sidebar">
          <PagesSidebar
            pages={book?.pages || []}
            selectedPage={selectedPage}
            onSelectPage={setSelectedPage}
          />
        </div>
      </div>

      {/* Bottom: Guidelines (conditional) */}
      {book?.status === 'guidelines_pending_review' && (
        <GuidelinesPanel
          bookId={bookId}
          onApprove={loadBook}
          onReject={loadBook}
        />
      )}
    </div>
  );
}
```

### 6.2 PageUploadPanel.tsx

```tsx
// llm-frontend/src/pages/admin/components/PageUploadPanel.tsx
import { useState } from 'react';
import { adminApi } from '../../../api/adminApi';
import ImageTextViewer from './ImageTextViewer';

export default function PageUploadPanel({ bookId, onPageApproved }) {
  const [uploadedImage, setUploadedImage] = useState(null);
  const [imagePreview, setImagePreview] = useState(null);
  const [ocrText, setOcrText] = useState('');
  const [pageNum, setPageNum] = useState(null);
  const [isProcessing, setIsProcessing] = useState(false);

  const handleFileSelect = async (e) => {
    const file = e.target.files[0];
    if (!file) return;

    // Show preview
    setUploadedImage(file);
    setImagePreview(URL.createObjectURL(file));

    // Upload and OCR
    setIsProcessing(true);
    try {
      const formData = new FormData();
      formData.append('image', file);

      const result = await adminApi.uploadPage(bookId, formData);
      setPageNum(result.page_num);
      setOcrText(result.ocr_text);
    } catch (error) {
      console.error('Upload failed:', error);
    } finally {
      setIsProcessing(false);
    }
  };

  const handleApprove = async () => {
    await adminApi.approvePage(bookId, pageNum);
    // Reset form
    setUploadedImage(null);
    setImagePreview(null);
    setOcrText('');
    setPageNum(null);
    onPageApproved();
  };

  const handleReject = async () => {
    await adminApi.deletePage(bookId, pageNum);
    // Reset form to allow re-upload
    setUploadedImage(null);
    setImagePreview(null);
    setOcrText('');
    setPageNum(null);
  };

  return (
    <div className="page-upload-panel">
      <h2>Upload Page</h2>

      {!uploadedImage ? (
        <div className="upload-dropzone">
          <input
            type="file"
            accept="image/*"
            onChange={handleFileSelect}
          />
          <p>Click or drag to upload page image</p>
        </div>
      ) : (
        <>
          {isProcessing ? (
            <div className="processing">
              <p>Processing OCR...</p>
            </div>
          ) : (
            <>
              <ImageTextViewer
                imageUrl={imagePreview}
                text={ocrText}
              />

              <div className="actions">
                <button onClick={handleApprove} className="approve">
                  Approve
                </button>
                <button onClick={handleReject} className="reject">
                  Reject & Re-upload
                </button>
              </div>
            </>
          )}
        </>
      )}
    </div>
  );
}
```

---

## 7. Dependencies & Blockers

### 7.1 Current Blockers

1. **AWS Credentials** ✅ RESOLVED
   - **Status:** Configured in `~/.aws/credentials`
   - **Region:** `us-east-1`
   - **boto3 auto-detection:** Credentials automatically picked up from default AWS credential chain
   - **No action needed:** Terraform already successfully using these credentials

2. **OpenAI Vision API Reference Code** ⛔ PENDING
   - Required for: Phase 4 (OCR integration)
   - Action: Awaiting reference implementation from user
   - Expected format: Python function using openai library

### 7.2 New Dependencies

**Backend (`requirements.txt`):**
```
boto3==1.34.0              # AWS S3 SDK
Pillow==10.1.0             # Image processing
python-multipart==0.0.6    # File upload support
```

**Frontend (`package.json`):**
```json
{
  "dependencies": {
    "react-router-dom": "^6.20.0",  // Already exists
    // No new deps needed - using built-in File API
  }
}
```

---

## 8. Timeline Estimates

**Assuming AWS credentials and Vision API reference provided:**

| Phase | Duration | Dependencies | Start Date | End Date |
|-------|----------|--------------|------------|----------|
| Phase 1: Database | 1-2 days | None | Day 1 | Day 2 |
| Phase 2: AWS S3 | 1 day | Phase 1, AWS creds | Day 3 | Day 3 |
| Phase 3: Book APIs | 2-3 days | Phase 1 | Day 2 | Day 4 |
| Phase 4: OCR | 2-3 days | Phase 2, 3, Vision API | Day 4 | Day 6 |
| Phase 5: Frontend | 3-4 days | Phase 3, 4 | Day 5 | Day 8 |
| Phase 6: LangGraph | 3-4 days | Phase 4 | Day 7 | Day 10 |
| Phase 7: Testing | 2-3 days | All phases | Day 11 | Day 13 |

**Total Estimated Duration:** 13-15 working days (~3 weeks)

**Can Start Immediately:**
- Phase 1 (Database) - No blockers
- Phase 3 (Book APIs) - Can mock S3 calls initially

**Critical Path:**
Phase 1 → Phase 2 → Phase 4 → Phase 6 → Phase 7

---

## 9. Testing Strategy

### 9.1 Unit Tests
- All services: 100% coverage
- All repositories: 100% coverage
- Mock external dependencies (S3, OpenAI)

### 9.2 Integration Tests
- API endpoints with real database
- S3 operations with mocked boto3
- LangGraph execution with mocked LLM

### 9.3 E2E Test
**Scenario:** Upload NCERT Math Magic Grade 3

1. Create book via admin UI
2. Upload 10 sample pages
3. Approve all pages
4. Generate guidelines
5. Approve guidelines
6. Verify teaching_guidelines table populated
7. Verify AI tutor can query new guidelines
8. Create teaching session with new guideline
9. Verify tutor can teach using book content

### 9.4 Performance Benchmarks
- Page upload + OCR: < 10 seconds
- Guideline generation (50 pages): < 5 minutes
- S3 upload (1MB image): < 2 seconds

---

## 10. Deployment Considerations

### 10.1 Environment Variables
Add to `.env` (AWS credentials auto-detected from `~/.aws/credentials`):
```
# AWS Configuration (credentials in ~/.aws/credentials)
AWS_REGION=us-east-1
AWS_S3_BUCKET=learnlikemagic-books
```

### 10.2 Database Migration
Run Alembic migration before deploying:
```bash
cd llm-backend
alembic upgrade head
```

### 10.3 S3 Bucket Setup
- Create bucket: `learnlikemagic-books`
- Enable versioning (optional)
- Set CORS policy for frontend uploads
- Configure lifecycle rules (optional, for cleanup)

### 10.4 IAM Permissions
For production, use IAM role with these permissions:
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObject",
        "s3:DeleteObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::learnlikemagic-books",
        "arn:aws:s3:::learnlikemagic-books/*"
      ]
    }
  ]
}
```

---

## 11. Risks & Mitigations

### 11.1 OCR Accuracy
**Risk:** OpenAI Vision API may not perfectly extract mathematical notation
**Mitigation:**
- Allow admin to reject and re-upload
- Post-MVP: Add inline editing
- Consider hybrid approach with specialized math OCR

### 11.2 Guideline Quality
**Risk:** AI-generated guidelines may be incomplete or inaccurate
**Mitigation:**
- Admin review required before approval
- Reject/retry mechanism
- Iterate on prompts based on testing

### 11.3 S3 Costs
**Risk:** Large number of high-res images could increase storage costs
**Mitigation:**
- Compress images before upload (Pillow)
- Implement S3 lifecycle policies to archive old books
- Monitor costs via AWS billing alerts

### 11.4 LangGraph Execution Time
**Risk:** Processing 50+ page book may take too long
**Mitigation:**
- Run asynchronously (202 Accepted response)
- Show progress indicator in UI
- Consider pagination/chunking for very large books

---

## 12. Success Criteria

### MVP is considered successful when:

1. ✅ Admin can log in to `/admin`
2. ✅ Admin can create a new book with metadata
3. ✅ Admin can upload 20+ pages sequentially
4. ✅ OCR extracts text with >90% accuracy (manual verification)
5. ✅ Admin can approve pages and see them in sidebar
6. ✅ Admin can mark book complete and trigger guideline generation
7. ✅ LangGraph generates guideline.json with all required fields
8. ✅ Admin can review and approve guideline
9. ✅ teaching_guidelines table populated correctly (verify DB query)
10. ✅ Existing AI tutor can use new guidelines to teach (integration test)
11. ✅ Full NCERT Math Magic book processed end-to-end
12. ✅ All unit and integration tests passing
13. ✅ No major bugs in UI or backend
14. ✅ System can handle re-uploads, rejections, and retries

---

## 13. Next Steps

1. ✅ **AWS Credentials:** Configured in `~/.aws/credentials` (auto-detected by boto3)
2. ⏸️ **Awaiting:** OpenAI Vision API reference code
3. **Week 1:** Complete Phase 1 (Database) + Phase 2 (S3) + Phase 3 (APIs)
4. **Week 2:** Complete Phase 4 (OCR) + Phase 5 (Frontend)
5. **Week 3:** Complete Phase 6 (LangGraph) + Phase 7 (Testing)
6. **Deploy:** Run migrations, deploy to production, test with real book

---

*Document last updated: 2025-10-25*
*Status: Ready to start Phase 1 & 2. Awaiting OpenAI Vision reference for Phase 4.*
