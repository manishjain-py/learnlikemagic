# Book Ingestion Pipeline Audit Report

**Date:** 2026-03-01
**Branch:** claude/audit-ingestion-pipeline-TpnLJ @ 74ebce3
**Auditor:** Claude Code (automated)

## Executive Summary

- Total findings: **27**
- **Critical: 3** | **High: 8** | **Medium: 10** | **Low: 6**
- Top 3 risks to study plan quality:
  1. **[CRITICAL]** No quality validation on V2 guidelines — QualityGatesService is parked, zero automated checks on extracted content
  2. **[CRITICAL]** Stability logic is broken — `_check_and_mark_stable_subtopics` loads shards/saves but never actually updates index status to "stable", so topics stay "open" indefinitely
  3. **[CRITICAL]** Guidelines merge token limit (1500) can silently truncate accumulated guidelines for content-heavy subtopics spanning many pages

---

## Findings by Category

### Category 1: Content Loss & Silent Truncation

| # | Severity | Location | Description | Impact | Recommendation |
|---|----------|----------|-------------|--------|----------------|
| 1.1 | **HIGH** | `minisummary_service.py:82` | `page_text[:3000]` hard truncation. Dense textbook pages (math with many problems, science with diagrams) can easily exceed 3000 chars after OCR. | Minisummary loses tail content — downstream context pack gets incomplete page summary. Boundary detection still sees full text, creating inconsistency. | Increase to 6000 chars or remove limit (summary prompt already controls output length). Log when truncation occurs. |
| 1.2 | **CRITICAL** | `guideline_merge_service.py:38` | `max_tokens=1500` for merged guidelines. After merging 5+ pages, accumulated guidelines can exceed 1500 tokens (~1100 words). The LLM response will be silently truncated by the token limit. | Content from later pages gets dropped after several merge iterations. A 10-page subtopic could lose 30-40% of its guidelines content. | Increase to 3000+ tokens or dynamically calculate based on input size. Add post-merge length validation. |
| 1.3 | **HIGH** | `boundary_detection_service.py:55` | `max_tokens=1000` for combined boundary decision + guidelines extraction JSON. The `page_guidelines` field must fit within this limit alongside `is_new_topic`, `topic_name`, `subtopic_name`, and `reasoning`. | Dense pages lose guideline content. The JSON overhead (~100 tokens for other fields + reasoning) leaves only ~900 tokens for actual guidelines. | Increase to 2000 tokens. The guidelines field is the primary value — it should not be constrained by metadata. |
| 1.4 | **HIGH** | `topic_deduplication_service.py:165` | `shard.guidelines[:200]` preview for dedup analysis. 200 chars (~30 words) is far too little to distinguish similar subtopics. | False positive dedup merges (distinct subtopics with similar intros get merged) or false negatives (truly duplicate subtopics with different intros survive). | Increase to 500-800 chars. This is the only signal the LLM has to decide if subtopics are truly duplicates. |
| 1.5 | **MEDIUM** | `boundary_detection_service.py:197` | `guidelines[:300]` preview in context pack for boundary detection. Each subtopic's guideline text is truncated to 300 chars in the prompt. | Boundary detection may not have enough context to recognize that a page continues an existing subtopic, leading to over-segmentation (creating duplicate subtopics). | Increase to 500-800 chars or send full text for the most recent 2-3 subtopics. |
| 1.6 | **MEDIUM** | `topic_name_refinement_service.py:64` | `guidelines[:2000]` truncation for name refinement. | Name refinement sees incomplete picture for large subtopics, potentially producing inaccurate names. Less impactful than others since names are cosmetic. | Increase to 3000 chars or use the subtopic_summary instead if available. |
| 1.7 | **MEDIUM** | `topic_subtopic_summary_service.py:47` | `guidelines[:max_chars]` where `max_chars=3000` for subtopic summary generation. | Subtopic summary may miss content from tail of long guidelines. Summaries feed into topic summaries, cascading the loss. | Acceptable for most cases; log when truncation occurs. |
| 1.8 | **LOW** | `ocr_service.py:34` | `max_tokens=4096` for OCR. Generally sufficient but very dense pages with extensive text, tables, and formulas could exceed this. | Extremely rare, but could lose content at page bottom. OCR output is the source of truth for all downstream processing. | Monitor actual OCR output lengths. 4096 tokens is usually adequate. |
| 1.9 | **MEDIUM** | `guideline_merge_service.py:139-143` | On LLM failure, fallback is simple concatenation: `f"{existing}\n\n{new}"`. This preserves all content but produces poorly organized, duplicative text. | After multiple fallback merges, guidelines become a disorganized wall of text with repeated information. Tutor and study plan generator receive low-quality input. | Log concatenation fallbacks prominently. Consider a retry before falling back. Add a "concatenated_fallback" flag to the shard. |

### Category 2: Topic Boundary Accuracy

| # | Severity | Location | Description | Impact | Recommendation |
|---|----------|----------|-------------|--------|----------------|
| 2.1 | **CRITICAL** | `guideline_extraction_orchestrator.py:599-636` | `_check_and_mark_stable_subtopics` loads each shard, updates `updated_at`, and saves it — but **never updates the index status** to "stable". The code has `# self._update_index_status(...)` commented out. So subtopics remain "open" in the index permanently, even after 5-page gap. | All topics appear as "open" in the context pack forever. The context pack grows without bound, eventually consuming excessive prompt tokens. The stability feature is essentially non-functional. | Uncomment or re-implement the index status update. After marking shard timestamp, update the corresponding `SubtopicIndexEntry.status` to "stable" and save the index. |
| 2.2 | **HIGH** | `context_pack_service.py:173-218` | `_extract_open_topics` iterates ALL topics (not filtered by status). The method name says "open" but the loop has no status filter — it loads every subtopic for every topic regardless of status. | Every subtopic is always in the context pack regardless of status. With many topics, the boundary detection prompt becomes very long, potentially exceeding token limits or degrading LLM quality. Partially mitigated by 2.1 (since nothing is actually marked stable), but architecturally wrong. | Add status filtering: only include subtopics with `status in ["open"]` (or "open" + "stable" for re-merge capability). For stable/final topics, include only the summary, not full guidelines. |
| 2.3 | **MEDIUM** | `boundary_detection.txt` (prompt) | No guidance for handling non-content pages (table of contents, preface, index, blank pages, exercise answer keys). The prompt assumes every page has educational content. | Non-content pages get forced into a topic, polluting guidelines with irrelevant content (e.g., table of contents entries become "guidelines"). | Add prompt guidance: "If the page is a table of contents, index, copyright page, or other non-educational content, set `page_guidelines` to empty string and `is_new_topic` to false." |
| 2.4 | **MEDIUM** | `boundary_detection.txt` (prompt) | No guidance for pages that span two distinct topics (e.g., last 3 paragraphs of Topic A, first paragraphs of Topic B). The prompt asks for a single topic/subtopic decision. | Content from the minority topic on a split page is either lost or incorrectly attributed to the majority topic. | Add guidance: "If a page clearly spans two topics, assign it to the topic with more content on this page. Note the split in your reasoning." |
| 2.5 | **HIGH** | `guideline_extraction_orchestrator.py:599-636` | Stability threshold is 5 pages (`STABILITY_THRESHOLD = 5`), but even if stability worked, a topic that reappears later in the book (e.g., Chapter 1: "Fractions intro", Chapter 5: "Advanced Fractions") would create a duplicate subtopic since the original is already stable/final. | Content that spans non-contiguous book sections creates redundant subtopics instead of being consolidated. The dedup step might catch this, but it relies on only 200-char previews. | Consider keeping stable topics in context with a "previously covered" label, so the LLM can choose to reopen them. Or rely on dedup with better preview length. |

### Category 3: Guideline Quality & Completeness

| # | Severity | Location | Description | Impact | Recommendation |
|---|----------|----------|-------------|--------|----------------|
| 3.1 | **CRITICAL** | `guideline_extraction_orchestrator.py:8` | `# No QualityGatesService (parked for V2)` — The V1 quality gates service validated extracted content (minimum objectives, examples, misconceptions, assessments, teaching description quality). V2 removed ALL quality validation. | Zero automated quality checks on extracted guidelines. A subtopic could have empty, incomplete, or nonsensical guidelines and proceed through finalization to the tutor. | Implement V2 quality gates: minimum guidelines length (>100 chars), presence of key pedagogical elements (objectives, examples), guidelines-to-page-count ratio check. |
| 3.2 | **HIGH** | `boundary_detection.txt:34-40` | Guidelines extraction prompt asks for "learning objectives, examples, teaching strategies, misconceptions, assessments" but provides no structure, no minimum requirements, and no examples of good output. | LLM may extract sparse, incomplete guidelines. Some pages might produce only "This page discusses fractions" instead of detailed pedagogical content. Quality is entirely at the LLM's discretion. | Add few-shot examples showing good vs. bad guideline extraction. Add minimum content expectations (e.g., "at least 100 words of guidelines"). |
| 3.3 | **MEDIUM** | `boundary_detection.txt:41` | `"Format as a comprehensive paragraph or bulleted text (your choice - natural language)"` — No enforced structure. | Inconsistent format across pages makes LLM merging harder and tutor consumption less reliable. Some pages get paragraphs, others get bullets. | Standardize on one format (bulleted list with categories) to make merging more consistent. |
| 3.4 | **HIGH** | `guideline_merge_v2.txt` | Merge prompt says "Keep all unique objectives, examples, misconceptions" but the LLM has no way to verify completeness. After 5+ merges, early content gets progressively summarized. | Progressive information loss — each merge iteration slightly condenses existing content to fit new content. After 10 pages, original guidelines from page 1 may be reduced to a single line. | Add post-merge validation: compare word count of merged output to sum of inputs. If merged < 60% of combined input, flag for review. |
| 3.5 | **LOW** | `guideline_merge_v2.txt:23` | Merge prompt says "Organize logically (objectives → teaching strategies → examples → misconceptions → assessments)" but boundary detection prompt says "your choice". | First extraction is unstructured, merge attempts to reorganize. This can cause content reshuffling that confuses the LLM. | Align both prompts: either both enforce structure or both allow freeform. |

### Category 4: Data Integrity

| # | Severity | Location | Description | Impact | Recommendation |
|---|----------|----------|-------------|--------|----------------|
| 4.1 | **HIGH** | `db_sync_service.py:399-401` | Full snapshot sync does `DELETE FROM teaching_guidelines WHERE book_id = :book_id` then inserts all shards. The delete happens immediately, but if the insert loop fails partway through (e.g., DB connection lost after 5 of 20 inserts), committed deletes cannot be rolled back. Note: line 401 does `DELETE` then line 403-424 loops inserts, with `commit()` at line 425. | Partial failure during sync can delete all existing guidelines while only inserting a subset. Previously approved guidelines and their study plans are lost. | Wrap the entire DELETE + INSERT loop in a single transaction (remove the intermediate `commit()` in `_insert_guideline`, do one `commit()` at the end). Alternatively, use a "soft delete" pattern. |
| 4.2 | **MEDIUM** | `db_sync_service.py:203-205` and `db_sync_service.py:272-274` | Individual `sync_shard` calls `self.db.commit()` after each insert/update. The `sync_book_guidelines` method calls `_insert_guideline` in a loop, and `_insert_guideline` commits individually. | Each shard commits independently. If the 10th shard fails, 9 are committed, 11 remain as old data. This creates a partially-synced state. The outer method catches this and rolls back, but the individual commits already happened. | Remove `commit()` from `_insert_guideline` and `_update_guideline`. Have `sync_book_guidelines` do a single `commit()` at the end (it already does at line 425, but individual inserts also commit). |
| 4.3 | **MEDIUM** | `guideline_extraction_orchestrator.py:498-524` | During finalization name refinement, if shard save succeeds but `_update_index_names` fails, the shard has new keys but the index still references old keys. | Index becomes inconsistent with actual S3 shard files. Subsequent operations that use the index to find shards will fail. | Make shard save + index update atomic, or implement an index rebuild/reconciliation step. |
| 4.4 | **LOW** | `study_plans/services/orchestrator.py:73-101` | Study plans are NOT invalidated when their source guideline is updated via re-sync. A re-generated guideline gets a fresh DB row (full snapshot sync deletes + reinserts), breaking the `guideline_id` FK in `study_plans`. | After re-syncing guidelines, existing study plans become orphaned (their `guideline_id` references a deleted row). The tutor may use stale study plans or crash on FK violations. | Add `ON DELETE CASCADE` to the `study_plans.guideline_id` FK. Or before re-sync, delete orphaned study plans. |
| 4.5 | **LOW** | `guideline_extraction_orchestrator.py:362-376` | When boundary detection says "continue" but the shard doesn't exist in S3 (e.g., due to a previous failed save), the orchestrator catches the exception and creates a new shard. | The "continue" decision means the LLM expected to merge with existing content, but the new shard only has this page's guidelines. Missing content from earlier pages that should have been there. | Log this as a warning with the topic/subtopic key. Consider re-extracting from the page that originally created the shard. |

### Category 5: Page Coverage

| # | Severity | Location | Description | Impact | Recommendation |
|---|----------|----------|-------------|--------|----------------|
| 5.1 | **HIGH** | `guideline_extraction_orchestrator.py:160-212` | Failed pages are logged but processing continues (`stats["errors"].append(error_msg)`). No mechanism exists to verify that all pages contributed to guidelines after extraction completes. | Pages that fail silently (e.g., empty OCR, LLM timeout) leave gaps in coverage. Those pages' content never makes it into any guideline. The admin has no easy way to know which pages are missing. | Add a post-extraction coverage report: compare `page_index.json` pages against total book pages. List uncovered pages prominently in the job completion status. |
| 5.2 | **MEDIUM** | `guideline_extraction_orchestrator.py:690-705` | `_load_page_text` tries two S3 paths (`{page_num:03d}.ocr.txt` and `{page_num}.txt`). If both fail, it raises an exception, which is caught by the page processing loop and logged as an error. | If OCR succeeded but saved to an unexpected path (edge case), the page is skipped entirely. The dual-path lookup is fragile. | Standardize on one OCR text path. Add metadata.json lookup to find the correct text path for each page. |
| 5.3 | **LOW** | No coverage verification endpoint exists | There is no API endpoint or admin UI view showing which pages are assigned to subtopics and which are unassigned. | Admin cannot verify coverage without manually inspecting S3 artifacts. | Add `GET /admin/books/{id}/coverage` endpoint that returns unassigned pages, or include coverage stats in the job completion response. |

### Category 6: Prompt Engineering Quality

| # | Severity | Location | Description | Impact | Recommendation |
|---|----------|----------|-------------|--------|----------------|
| 6.1 | **MEDIUM** | `boundary_detection.txt` | No few-shot examples. The prompt describes what to do but doesn't show examples of correct boundary decisions for CONTINUE vs. NEW. | LLM may make inconsistent decisions. Different pages with similar contexts might get different decisions depending on model temperature variation. | Add 2-3 few-shot examples showing: (a) page continuing same subtopic, (b) page starting new subtopic within same topic, (c) page starting entirely new topic. |
| 6.2 | **LOW** | `boundary_detection.txt:28-29` | Topic/subtopic naming instruction says "lowercase, kebab-case" but the LLM is also asked to provide `topic_name` which is then slugified by the code. If the LLM returns a title-case name, `slugify()` handles it, but mismatches between the LLM's `topic_name` and existing topics' keys cause false "new topic" decisions. | When the LLM returns "Data-Handling" instead of "data-handling", it doesn't match existing "data-handling" topic, creating a duplicate. The code slugifies, but the LLM has already decided it's a "new" topic. | The prompt already instructs "MUST exactly match" for CONTINUE. Consider adding a validation step: after slugifying, check if the key matches any existing topic and override `is_new_topic` to false. |
| 6.3 | **LOW** | `minisummary_v2.txt` | Minimal prompt with no system-level context about the book's grade, subject, or board. Summary quality could improve with context. | Summaries may be generic instead of grade-appropriate. A Grade 3 math page summary might use college-level language. | Add `{grade}`, `{subject}` placeholders to the prompt so the summary is contextually appropriate. |
| 6.4 | **LOW** | `guideline_merge_v2.txt` | No examples of good merging. The prompt lists requirements but doesn't show what a well-merged guideline looks like. | LLM may interpret "merge" differently across calls — sometimes summarizing, sometimes concatenating, sometimes restructuring. | Add one before/after example showing existing guidelines + new guidelines → merged output. |
| 6.5 | **MEDIUM** | `study_plan_generator.txt` | The prompt receives the full `guideline_text` without any length limit in the generator_service code (`guideline.guideline or guideline.description or ""`). For very long guidelines (3000+ chars after many merges), this could strain the study plan LLM context. | Not a truncation risk per se (modern LLMs handle long prompts), but very long guidelines may cause the study plan to be unfocused or miss key concepts buried in verbose text. | Consider sending a condensed version (subtopic_summary + key sections) for guidelines exceeding 2000 chars. |

---

## Live Data Audit Results

No book_id was provided. Skipping live data audit.

---

## Priority Fix List

Ordered by impact on study plan quality:

| # | Severity | Fix | Effort | Confidence | Rationale |
|---|----------|-----|--------|------------|-----------|
| 1 | CRITICAL | **Fix stability logic** — Uncomment/re-implement index status update in `_check_and_mark_stable_subtopics`, save index after marking. | S | **95%** | The code already has the logic written (commented out). The 5-page gap check works; only the index write is missing. Pure code path fix, no behavioral unknowns. |
| 2 | CRITICAL | **Increase merge token limit** — `max_tokens=1500` → `3000` in `GuidelineMergeService`. Add post-merge validation: warn if `len(merged) < 0.6 * (len(existing) + len(new))`. | S | **90%** | Doubling the limit directly addresses truncation. The OpenAI API respects `max_completion_tokens` — higher limits simply allow longer output without forcing it. The 0.6 ratio check is a heuristic; may need tuning after observing real merge ratios. |
| 3 | CRITICAL | **Implement V2 quality gates** — After boundary detection, validate: `len(page_guidelines) > 50`, `len(shard.guidelines) > 100` after merge. Log/flag violations. | M | **80%** | Length thresholds are effective for catching empty/garbage extraction. The exact thresholds (50/100 chars) need calibration against real data — too strict risks false positives on legitimately short topics. Start as warnings, not blockers. |
| 4 | HIGH | **Increase boundary detection token limit** — `max_tokens=1000` → `2000`. | S | **95%** | Same rationale as #2. The JSON response schema is well-defined; more tokens just allow longer `page_guidelines` and `reasoning` fields. No risk of degraded output. |
| 5 | HIGH | **Increase dedup preview** — `guidelines[:200]` → `guidelines[:600]` in `_build_topics_summary`. | S | **85%** | 600 chars captures 2-3 paragraphs, enough for semantic comparison. Risk: larger context increases LLM cost per dedup call. Confidence not 95% because dedup accuracy also depends on prompt quality, not just preview length. |
| 6 | HIGH | **Fix DB sync atomicity** — Remove `self.db.commit()` from `_insert_guideline` and `_update_guideline`. Rely on the single `commit()` at end of `sync_book_guidelines`. | S | **95%** | Standard transactional pattern. SQLAlchemy accumulates operations until explicit commit. The outer method already has a `rollback()` in its except block. Only risk: very large books (200+ shards) may hit memory pressure from accumulated uncommitted rows, but this is unlikely. |
| 7 | HIGH | **Add coverage report** — After extraction loop, compute `assigned_pages = set(page_index.pages.keys())`, compare to `range(start_page, end_page+1)`. Include `unassigned_pages` in job completion stats. | S | **95%** | Pure data comparison, no LLM dependency. Page index is already maintained during extraction. Only gap: pages that failed and were skipped are already tracked in `page_errors`, so this is mostly surfacing existing data. |
| 8 | HIGH | **Filter context pack by status** — In `_extract_open_topics`, add `if subtopic_entry.status not in ["open"]: continue`. For stable topics, include only summary (not full guidelines). | M | **75%** | Reduces prompt size and focuses LLM attention. Risk: if a topic reappears after being marked stable, the LLM won't see it and will create a duplicate. Mitigation: include stable topics with summary-only (not full guidelines). Needs testing with real books that revisit topics. |
| 9 | HIGH | **Add non-content page handling** — Add to boundary detection prompt: "If this page is a table of contents, index, copyright, or non-educational content, return empty `page_guidelines` and `is_new_topic: false`." | S | **70%** | Depends on LLM correctly identifying non-content pages. Most LLMs handle this well for obvious cases (TOC, copyright). Edge cases: exercise answer keys, appendices with formulas — these may be incorrectly classified. Needs prompt iteration. |
| 10 | HIGH | **Add extraction quality check** — After boundary detection returns, validate `len(page_guidelines.strip()) > 0`. If empty, log warning and skip merge (don't pollute shard with empty content). | M | **85%** | Simple guard that prevents the most damaging case (empty guidelines entering the shard). The check is deterministic. Risk: some pages legitimately have minimal extractable content (e.g., a page with only a diagram). |
| 11 | MEDIUM | **Increase context guidelines preview** — `guidelines[:300]` → `guidelines[:600]` in `_build_prompt`. | S | **80%** | Same logic as #5. More context helps LLM match pages to existing subtopics. Risk: longer prompts increase cost and may push total prompt past context window for books with many open topics. |
| 12 | MEDIUM | **Add few-shot examples to boundary detection prompt** — Add 2-3 examples showing CONTINUE vs NEW decisions with sample inputs/outputs. | M | **70%** | Few-shot examples generally improve LLM consistency. Risk: examples may bias the LLM toward patterns in the examples rather than the actual content. Needs careful selection of diverse, representative examples. |
| 13 | MEDIUM | **Fix study plan orphaning** — Add `DELETE FROM study_plans WHERE guideline_id IN (SELECT id FROM teaching_guidelines WHERE book_id = :book_id)` before the guidelines DELETE in `sync_book_guidelines`. | S | **95%** | Pure SQL cascade. Deterministic, no edge cases. Alternative: add `ON DELETE CASCADE` FK constraint at the schema level, but that requires a migration. |

---

## Architecture Recommendations

Long-term suggestions for pipeline resilience:

1. **Add an end-to-end pipeline test** — Create a small test book (5-10 pages) with known content. Run the full pipeline and assert: all pages assigned, no empty shards, study plans generated, guidelines contain expected keywords. This catches regressions automatically.

2. **Implement progressive quality scoring** — After each page merge, compute a quality score (guidelines length / pages covered, presence of key sections). Flag subtopics below threshold for human review. This replaces the parked QualityGatesService with a V2-appropriate approach.

3. **Add pipeline observability dashboard** — Surface key metrics per book: pages covered, average guidelines length per subtopic, merge fallback count, dedup merge count, empty shard count. Currently these are only in logs.

4. **Consider two-pass extraction** — Pass 1: Rapid boundary detection + rough guidelines. Pass 2: Full guidelines extraction with the known topic structure. This separates the "structure discovery" concern from "content extraction" and reduces the token limit pressure on the combined boundary+extraction call.

5. **Decouple context window from S3 reads** — Currently, each page's context pack makes N S3 reads (one per open subtopic shard). For a book with 20 subtopics, this is 20+ S3 reads per page. Consider caching the index and shard data in memory during extraction (they're already processed sequentially in one thread).

6. **Add idempotent DB sync** — Instead of DELETE + INSERT, use UPSERT based on `(book_id, topic_key, subtopic_key)`. This preserves approved guidelines that haven't changed, prevents data loss on partial failure, and preserves study plan foreign keys.
